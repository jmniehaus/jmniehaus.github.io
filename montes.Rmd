---
title: "Monte Carlo Analysis in R: An in Depth Guide, with Examples"
date: "Updated: `r format(Sys.time(), '%d %b, %Y')`"
pagetitle: "Monte Carlos"
header-includes:
- \usepackage{mathtools}
- \usepackage{amsmath}
output:
  html_document:
    toc: yes
    number_sections: no
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: show
bibliography: bibliography.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---


```{r, echo=F, eval=T}
knitr::opts_chunk$set(class.output = "bg-success",
                      class.source = "fold-show",
                      tidy = F,
                      echo=T, 
                      eval=T,
                      message=F,
                      cache=T)

```

\


After quite a few iterations of writing Monte Carlo simulations, I've decided to compile a post on how to get started doing montes, all without having to endure some of the painful mistakes that I made. These mistakes include (but are not limited to):

1. Serial simulations/loops
2. Ignoring implicit parallelization
3. Improper seed-setting
4. Nested loops
5. Ignoring load balancing
6. Ignoring copy-on-modify overhead
7. Improper version control
8. Not using functions, or enough of them
9. Using Windows operating system

First, I will walk through some of the details, with toy examples to drive points home where applicable. Finally, I will take all of this and put it together in a more serious application where we explore the bias of logistic regression in small samples, as well as a penalized likelihood approach to solving this bias. 

# Serial vs Parallel
***

If you're working in R, then implementing parallel simulations is somewhat trivial to do. As such, if you are not using parallel processing to get your sims done faster, there are obvious efficiency gains to be had. There are exceptions to this rule (e.g., reserving computing resources for other tasks, parallel overhead making things slower), but if the reason you are not using parallel processing is simply because you do not know how, then I'll walk you through a few approaches here. *Note, however, that I assume your simulations are trivially parallelizable*, meaning that each simulation is completely independent of the others. If this is not true, then this walk-through is not necessarily going to apply to your case.

For going parallel, there are essentially two options:

1. Using `foreach` loops, which are similar, yet distinct from the typical `for` loop

2. Using the `apply` family of functions' `mclapply` or `parLapply` from the `parallel` package, which is installed by default in R.

I've used both approaches extensively (in the past I only used the `apply` approach), and strongly prefer the `foreach` approach at this point for a few reasons:

  a. It tends to be more intuitive as its similar to `for` loops in appearance
  b. Tends to be easier to debug (in my opinion)
  c. Doesn't require nested function calls for complicated simulations
  d. Permits nested parallel looping
  
For these reasons, I won't be covering the `apply` family of functions here. I won't go into the details on my reasoning for the sake of space and attention, but there are plenty of guides elsewhere on those functions.


### Differences between `foreach` and `for`

On the surface, the `foreach` package seems to implement a `for`-loop. I say "on the surface" because after every call to foreach you'll see something like `%dopar%`, which is an [infix function](https://adv-r.hadley.nz/functions.html#function-forms), similar to matrix multiplication using `%*%`. Thus, **foreach has a return value because it is a function.** In contrast, an actual `for` loop simply executes a set of instructions repeatedly, with no explicit return value. If this is not intuitive right now, this example may help:

```{r, echo=T, eval=T}
#for loop
vec = seq(1, 100)
out_for = vector()

for(i in seq_along(vec)) 
  out_for[i] = vec[i] + 1

#====================================================
#foreach 
invisible(
  library(foreach)
  )

out_foreach = foreach(i = seq_along(vec),
                      .combine = "c"
                      ) %do% {
                        
    return(vec[i] + 1)
  }

```

If you aren't familiar with the syntax of `foreach` yet, you'll want to take a look at its [vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html). However, notice that the above two loops (which we would never actually want to run like this, as addition is vectorized in R) differ. The `for` loop is just executing a set of instructions in each iteration, replacing elements of `out_for`. In contrast, the `foreach` function is actually *two functions*:
  1. The `foreach()` call
  2. The `%do%` call, which is an infix function
  
In other words, the call to `foreach()` creates an object on the left-hand side of the infix function `%do%`, which then also takes in an expression on the right-hand side following the curly brace `{`. This is conceptually similar to matrix multiplication in R, e.g. `X %*% Z` for conformable matrices `X` and `Z`; the arguments to the matrix-multiplication function `%*%` are `X` and `Z`, and to `%do%` they are an object returned by `foreach()`, and an `expression` on the right hand side. If this isn't making a whole lot of sense, I highly recommend reading up on infix functions from the link above, and perhaps looking into [expressions](https://adv-r.hadley.nz/expressions.html). 

Why does this difference matter? The main reason is that **because `%do%` is a function, it has a return value,** unlike the typical `for` loop which executes a series of instructions on objects but has no return value itself. That is why we have to store the results of the `foreach` loop in an object explicitly, `out_foreach`. Why this difference exists will be explained in the next section.

### Making `foreach` Run in Parallel

Now that you have the basics of how `foreach` and `for` differ, we can move on to making things work in parallel. The above toy example is in fact running serially in both the `foreach` and `for` loops. However, to make the `foreach` loop run in parallel requires trivial alterations:

1. Tell R how many cores to use

2. Register those cores as a parallel backend using the `doParallel` package, which imports `foreach`

3. Change `%do%` to `%dopar%` (though later we'll see that a better alternative is actually `%dorng%`)

4. Shut down the parallel backend (**this is extremely important for PC longevity**)

```{r, echo=T, eval=T}
library(doParallel)

# 1. Use one fewer cores than max available due to using R studio
ncores = parallel::detectCores() - 1 

# 2. spin up the cluster
cl = makeCluster(ncores)
registerDoParallel(cl)

# 3. dopar 
out_foreach = foreach(i = seq_along(vec),
                      .combine = "c"
                      ) %dopar% {
                        
    return(vec[i] + 1)
  }

#4.shut down the cluster DONT FORGET THIS
stopCluster(cl)
```

Now of course this example is contrived, and is only intended to show how to write the instructions. A full example implementing all advice in this page is available at the end. 

Perhaps you can intuit now why `%dopar%` is actually an infix function: the instructions in the `expression` on the right hand side (that is, the loop code) have to be passed to every single core that we are using so that they know what code to execute, *and* the object returned by `foreach(...)` has to be passed as well in order to keep track of iterations etc. This is because each core has its own instance of R running in the background executing a chunk of the simulations, so each of them is passed all the necessary instructions. 


\
\

# Seed Setting and Parallelism
***

If you're running simulations, you probably want to be able to reproduce them, either on your own machine at a later date, or on another machine. However, if you've now implemented parallel processing as suggested using `%dopar%`, setting your seed is no longer as straightforward as in the serial case. For example, lets simulate some random variables using both a serial loop and parallel loop, and set the seed before hand. We'd expect that the results are the same, but this turns out to be false.

First, serially:

```{r, echo=T, eval=T}
# Serially
serial_1 = serial_2 = c()

set.seed(123)
for(i in 1:3) 
  serial_1[i] = rnorm(1)

set.seed(123)
for(i in 1:3) 
  serial_2[i] = rnorm(1)

identical(serial_1, serial_2)
```

which produces two vectors of the exact same numbers, as we'd expect given the above code. 

However, when we implement the "equivalent" in parallel (notice how the infix property becomes more apparent in the code below, too):

```{r, echo=T, eval=T, class.output = "bg-danger",}
# Parallel
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

set.seed(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

we end up with results which are not the same, despite setting the seed as one would typically do in the serial case. The details of why this is happening are technical and mostly unimportant; what is important is that there are some simple solutions.

### Option 1: `doRNG`

The simplest solution is to use the [`doRNG`](https://cran.r-project.org/web/packages/doRNG/vignettes/doRNG.pdf) package. There are two ways to use the package to reproduce simulation results. 

1. **`doRNG` Method 1**: Set your seed as usual, and replace `%dopar%` with `%dorng%`:

```{r, echo=T, eval=T, message=F}
# set.seed and  %dorng%
library(doRNG)
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dorng% { rnorm(1) }

set.seed(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dorng% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

\
2. **`doRNG` Method 2**: Register your back-end using `registerDoRNG()`, then use `%dopar%` as usual:

```{r, echo=T, eval=T, message=F}
# doRNG backend, %dopar%
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

registerDoRNG(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

registerDoRNG(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

Both approaches result in reproducible parallel loops. 

### Option 2: Core-level Seeds

A bit of a hacky alternative to `doRNG` is to simply set the seed on each core. Remember from earlier that the instructions set on the right-hand side of `%dopar%` are passed to each core. Well, if each core has a unique seed, then any time we run the loops, the same results will obtain. To do this we 

1. Set an "outer" seed in the traditional way. 

2. Sample "inner" seeds to be used in each set of parallel instructions. The number of inner seeds is equal to the number of simulations.

3. Set the seed within your simulation code using `set.seed()`, but index your inner seeds using the iteration number for the simulation.

For example:

```{r, eval=T, echo=T, results='hide'}
ncores = 3
nsims = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)
inner_seeds = sample(
  x = 999999, 
  size = nsims
  )

parallel_1 = 
  foreach(i = 1:nsims, 
          .combine = c) %dopar% {
            
    set.seed(inner_seeds[i])
    rnorm(1) 
    
  }

parallel_2 = 
  foreach(i = 1:nsims, 
          .combine = c) %dopar% {
            
    set.seed(inner_seeds[i])
    rnorm(1) 
  }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

Now this is of course a bit hacky, but this approach does have practical utility. Namely, if you ever need nested parallel loops (using the `%:%` nesting operator detailed in the `foreach` documentation), then unfortunately the `%dorng%` backend is not supported. In this case, the only way I have found to successfully reproduce simulations is the above, core-level seed-setting approach. To be clear, this isn't setting the same exact seed on all cores. We made a vector of seeds, `inner_seeds`, and each simulation uses a seed from this vector before executing its instructions. You can think of this as setting a different seed across a bunch of different R sessions, and then combining the results; however, the universe of inner seeds remains the same, and therefore so does the results set. 


\
\

# Thread Competition
***

Now we are running loops in parallel, and our pseudo-random numbers are reproducible as we would want. However, there is a potential lurking issue that will completely tank any simulations structured as we have here: other multithreaded/parallel tasks competing for CPU resources. In other words, if R is trying to execute simulations in parallel while also trying to execute some other task in parallel simultaneously, often times your computer will hang and eventually crash. 

So why would anyone be doing multiple forms of parallel processing simultaneously? The most common reason for this is when you are using optimized BLAS/LAPACK libraries. That is, when you've installed some alternative, optimized linear algebra library and linked it up with R, such as [Open BLAS](https://www.openblas.net/), [MKL](https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html), or [Atlas BLAS](http://math-atlas.sourceforge.net/). In addition to having superior algorithms that are optimized for your specific hardware, these libraries often times have multithreaded matrix operations; ie, they do common matrix operations in parallel, such as matrix multiplication, inversion, etc. Thus, if you are using these libraries and forget to disable their inherent multithreading, they will compete with `foreach` when accessing your CPUs, which will ultimately hang your system. For example, if your computer has 8 threads, and you tell R to give `foreach` 7 of them, but MKL reserves 4 of them for matrix operations, suddenly you're trying to access 11 threads when you only have 8. The only option is to use the same thread for multiple tasks simultaneously, but this isn't possible so the computer hangs. 

\

### Checking BLAS/LAPACK Threading

To check if your PC is running multi-threaded matrix operations, use the [`RhpcBLASctl`](https://cran.r-project.org/web/packages/RhpcBLASctl/index.html) package:
```{r, echo=T, eval=T}
library(RhpcBLASctl)
blas_get_num_procs()
omp_get_num_procs()
```

If either of these results is greater than 1, then you have multithreading enabled. We won't delve into the differences between BLAS and OMP here; suffice it so say that you want both of these at 1 for single-threaded matrix operations. <span style="color: red;">**IF YOU USE [MICROSOFT'S R-OPEN](https://mran.microsoft.com/open)**, **_then you definitely have MKL enabled as your BLAS/LAPACK library, and multi-threading is likely enabled by default_**</span>. 

Alternatively, you can check on the name of your BLAS library, which will tell you at least if its not the default library:

```{r, echo=T, eval=T}
sessionInfo()[c("BLAS", "LAPACK")]
```

but this isn't conclusive, as it merely tells us that I'm using MKL, not that MKL is currently set to use multiple threads. For that reason, the `RhpcBLASctl` method is probably superior.

\

### Multi-threaded Packages
In addition to multithreaded matrix operations, there are other R packages that are multithreaded by default and will therefore compete for CPU resources. The number one culprit here is probably [`data.table`](https://cran.r-project.org/web/packages/data.table/index.html). However, other machine learning libraries have the option for multithreading as well, such as [`caret`](https://topepo.github.io/caret/), and [`glmnet`](https://cran.r-project.org/web/packages/glmnet/index.html). To my knowledge neither of these last two are multithreaded by default, so this only becomes a problem if you accidentally tell them to use more than one CPU. 

\
  
### Solving Thread Competition
Fortunately, it is extremely easy to solve this problem.

1. To disable multithreaded matrix operations, use the [`RhpcBLASctl`](https://cran.r-project.org/web/packages/RhpcBLASctl/index.html) package, **and be sure to use the code within your parallel loops so that each child process disables the multi-threading.**

```{r, echo=T, eval=T, results='hide'}
# disable in parent process 
library(RhpcBLASctl)
library(doRNG)

blas_set_num_threads(1)
omp_set_num_threads(1)

ncores = 3 
nsims = 3
cl = makeCluster(ncores)
registerDoParallel(cl)
set.seed(123)

# disable in child processes 
  foreach(i = 1:nsims,
          .packages = "RhpcBLASctl"
          ) %dorng% {
            
    library(RhpcBLASctl)
    blas_set_num_threads(1)
    omp_set_num_threads(1)   
    
    rnorm(1) 
    
          }
  
stopCluster(cl)
```
<br>

2. To disable packaged multithreading, you'll just have to read the documentation for that package to determine how to set the max number of threads to 1. For example, browsing the documentation for `data.table` immediately tells us that the `setDTthreads()` function does what we want. 

\

### Caveats

There are a few qualifiers worth mentioning here. 

1. If you aren't using matrix operations in your parallel tasks, then multithreaded matrix operations shouldn't cause problems in theory, because they aren't even being attempted. However, if they aren't happening, it also doesn't hurt to just set the number of threads to 1. Just make sure the change it back if you want to leverage that efficiency in your R session later on. 

2. If you aren't using the multithreaded R packages, perhaps it goes without saying that you don't need to worry about them competing for resources in this way.

3. It is possible that some combination of matrix or package multithreading along with parallel simulations is "optimal," in the sense that its faster. For example, if you have 16 threads you could assign 5 threads to `foreach`, and then have each of those processes use 2 threads for matrix operations. You'd then have 15 threads in use, plus one for R-studio or other tasks on reserve. Perhaps this would be faster overall than single-threaded matrix operations on 15 simulation processes. I haven't done any testing for this, so I have no idea whether or when this would be better. At the extreme, it could in theory be better to use a single threaded simulation (i.e., serial simulations), while using all cores for matrix operations. I imagine that is an edge case, but again I cannot rule it out as I have not investigated this extensively. In any case, I'm lazy, so I just disable multithreading on all packages and matrix operations, and give all my threads to `foreach`. 

# Parameter Grids 

***

Now that we're working in parallel without suffering thread competition and can consistently reproduce our results, the next step is to create a grid of parameters for our simulations, i.e., the points in the parameter space at which we'll investigate the behavior of our chosen procedure. These will of course be specific to the specific case of simulations under study, but its worth showing a general way to go about creating this grid. 

As a contrived toy example, suppose our task involves normal random variables with different means and variances. Specifically, we are interested in $X \sim \mathcal{N}\left(\mu \in \{0, -1, 1, -3, 5\}, \,\,\sigma^2 \in \{1, 1.5, 3\}\right)$; that is, we'll vary the mean and variance of some random variable $X$. Right now its not important *why* we're doing this or what $X$ will be used for, only that we want random variables with every pairwise combination of these parameter values. 

To do so, we create vectors of these parameters, and then use `expand.grid()` to get the pairwise combinations:

```{r}
mu = c(0, -1, 1, -3, 5)
sig2 = c(1, 1.5, 3)

grid = expand.grid(mu = mu, sig2 = sig2)
grid
```


So now we have the parameters we need for our simulations in a nice tabular format, which we can then loop over when creating data from our chosen data generating process. For example:

```{r}
library(doParallel)
library(RhpcBLASctl)
library(doRNG)

# Disable thread competition
blas_set_num_threads(1)
omp_set_num_threads(1)

# Make grid
mu = c(0, -1, 1, -3, 5)
sig2 = c(1, 1.5, 3)
grid = expand.grid(mu = mu, sig2 = sig2)

# Make backend
ncores = parallel::detectCores() - 1
cl = makeCluster(ncores)
registerDoParallel(cl)

# Set seed and use dorng
set.seed(123)


out = foreach(i = 1:nrow(grid),
        .packages = "RhpcBLASctl",
        .combine = c
        ) %dorng% {
            
    library(RhpcBLASctl)
    blas_set_num_threads(1)
    omp_set_num_threads(1)   
    
    rnorm( n = 1,
           mean = grid[i, "mu"],
           sd = sqrt(grid[i, "sig2"])
    )
    
  }
  
stopCluster(cl)
attributes(out) = NULL # makes print look cleaner, removes rngstream attr

out
```

All we've done here is create the grid as seen earlier, loop over each row indexed by `i`, and simulated one draw from the specified parameterization of the normal distribution from `grid`. In this way we've ensured that we simulate data from each of the desired parameterizations, all without having to manually specify the parameters, and without having to do any sort of nested looping, e.g. `for(i in mu){ for j in sig2 {...}}`. This is akin to a grid-search in optimization, although here we're not searching for an optimal set of parameters, we're investigating the behavior of some procedure at pre-defined grid points in the parameter space. 

Of course, *we've only done one simulation* here. That is, at each set of parameter values, we have one iteration. But in Monte Carlo analysis, we want some large number of simulations in order to see how things are behaving, so we'll need to replicate this grid. Suppose we want 2 simulations instead of 1, then we can use `do.call()`, `cbind.data.frame`, and `replicate()` together to get a grid with each grid point appearing twice, so that we'll have two simulations at that point instead of one:

```{r}
nsims = 2 
mu = c(0, -1, 1, -3, 5)
sig2 = c(1, 1.5, 3)

grid = expand.grid(mu = mu, sig2 = sig2)
grid_multi = do.call(rbind.data.frame, replicate(nsims, grid, simplify=F))
grid_multi
```

And we can now see that each grid point appears twice, so if we were to loop over this in the same as as we did previously, we produce results for each parameter combination twice. Inductively then, we know that for arbitrary values of `nsims`, we'll get `nsims`-many results for each parameter combination, which is precisely what we want in Monte Carlo analysis. 

Briefly explaining the above code, `replicate()` is replicating the `grid` dataframe `nsims`-many times, and storing these replicates in a list. Then, `do.call` calls the `rbind.data.frame` function with each element of the replicated list as its arguments. Mathematically, `do.call` is effectively executing $f(x_1, x_2, ..., x_{nsims})$ where $f()$ is `rbind.data.frame`, and each `x_j` is an element of a list. In this case, the list-elements are all the original, single-simulation `grid` object.  

### Why not use a nested loop? 

One potential question is why not use a nested loop instead of replicating each grid point `nsims`-many times. That is, we could just repeatedly loop over the original `grid` object, rather than replicating it several times, like so: 

```{r, echo=T, eval=F}
# BACKEND ETC OMITTED FOR CLARITY

# Make grid WITHOUT REPLICATING 
mu = c(0, -1, 1, -3, 5)
sig2 = c(1, 1.5, 3)

grid = expand.grid(mu = mu, sig2 = sig2)

# make number of simulations a variable for looping
nsims = 100

out = foreach(i = 1:nrow(grid), 
        .packages = "RhpcBLASctl",
        .combine = c
        ) %dorng% {
            
    res = c()
    
    # create an inner, nested loop that repeatedly simulates from a gridpoint
    # instead of replicating the gridpoint and looping over the replicates
    for(j in seq_len(nsims)){

      res[j] = rnorm( n = 1,
                      mean = grid[i, "mu"],
                      sd = sqrt(grid[i, "sig2"])
              )
    }
    
    
}
```

The key difference here is that we create a single set of grid points without duplication, and then create an inner loop (`for(j in seq_len(nsims))`) that repeatedly uses these grid points `nsims`-many times. An obvious advantage of the nested-loop approach is that it conserves memory; the grid is not replicated, so each core only stores a copy of the original, single-simulation grid.  The memory gains are somewhat trival though due to how R stores lists in memory, but that is a technical aside that I won't delve into now (see [this page](https://adv-r.hadley.nz/vectors-chap.html#lists) for more info). 


However, the major disadvantage of the nested-loop approach is load balancing. In most real-world applications we will be varying the sample size of our procedure; i.e., we'll estimate some model at small and large sample sizes. Those sample sizes will then appear in our grid because they are a hyper-parameter after all, which then induces severe heterogeneity in computation time for each CPU. For example:

```{r, eval=F}
# Omit backend, etc for clarity

nsims = 100
mu = c(0, -1, 1, -3, 5)
sig2 = c(1, 1.5, 3)
n = c(1, 100, 1000, 10000)

# Add sample size to grid
grid = expand.grid(mu = mu, sig2 = sig2, n = n)



out = foreach(i = 1:nrow(grid), 
        .packages = "RhpcBLASctl",
        .combine = c
        ) %dorng% {
            
    res = c()

    for(j in seq_len(nsims)){
        
          # No longer hard-coding sample size, we index it
          res[j] = rnorm( n = grid[i, "n"], 
                          mean = grid[i, "mu"],
                          sd = sqrt(grid[i, "sig2"])
              )
    }
```

So now we are not hard-coding the sample size, `n`, but we are varying it. In doing so, the nested-loop approach becomes problematic for load balancing purposes, because the CPUs that have the small sample sizes will take way less time to complete their inner, nested loop (recall, the set of instructions on the right-hand side of `%dopar%` are executed on a single CPU). Eventually then, the CPUs with the large sample sizes will still be running their nested, inner-loops while the CPUs that had the smaller sample sizes have completed their tasks and are *sitting idle* doing nothing. That all in mind, for the specific toy examples here it turns out that the nested approach is faster than the replicating approach due to parallel processing overhead being greater than the time it takes to complete a task (see [here](https://en.wikipedia.org/wiki/Parallel_slowdown) for more on this). However, for more interesting cases, this will generally not be true.

### Caveats 

In most cases, the replicated-grid approach will be better, often times by several hours when compared with the parallel-outer, serial-inner approach. However, there are at least two instances where this won't hold, one of which was alluded to in the previous paragraph. 

1. If the run-time of a single simulation is small, e.g., < 1 second, it is possible that the overhead from parallelizing at the simulation level--rather than the grid-point level--will exceed the gains from parallelizing. This is known as parallel slow down. Its possible to mitigate this by using prescheduling, but even this won't always fix the issue if single simulations take a very small amount of time.  Prescheduling splits the total number of tasks into chunks (at random) and assigns these tasks to each CPU. So if you allocate 10 CPUs, and there are 100 tasks, the computer will randomly select 10 tasks to assign to each CPU. Without prescheduling, when a CPU finishes a task, it will communicate with the other CPUs in the network or the parent process and determine what tasks still need doing, and then it does one of the ones that remains. So, by prescheduling we reduce some of this communication overhead. There are advantages and disadvantages to prescheduling that I won't go into here, but a general rule is if your tasks are numerous and require little time individually, use prescheduling, otherwise don't. To enable presheduling in `foreach`, do the following:

```{r, eval=F}

foreach(i = 1:nsims, 
        .options.multicore = list(preschedule = TRUE)) %dorng% {
          
          ...
        
        }
```

 

2. If you are doing some form of Markov Chain Monte Carlo (MCMC), rather than traditional Monte Carlo analysis, the nested loop approach will generally become **_necessary_**. For things like the Metropolis-Hastings algorithm and its derivatives (e.g., Metropolis, Gibbs), we are typically sampling from a full-conditional distribution that depends on some previous iteration. So parallelization here would occur at the chain-level, and within each chain we would have serial loops to sample from the full conditionals. The gridding approach would also not make as much sense anyway, as the proposal parameters are not fixed ex-ante aside from the starting values.




# Using Functions Properly 

***

At this point we've properly set up our environment, and are ready to start with the actual simulations. To recap, we are using `foreach` combined with `%dopar%` or `%dorng%` for parallelism, we've ensured reproducability, we have handled thread competition with other parallel processes, and now we have a grid of parameter values at which to investigate the behavior of some procedure, with each grid value appearing once for each simulation iteration. Now we can make some functions that will automate the simulation and transformation of our data.

R is an object oriented, functional programming language. We should leverage both of these facts to make our simulations easier to understand, implement, and debug. At a minimum, this will consist of having two functions: a simulation function that creates the data to be further analyzed, and an estimation/analysis function that estimates the model(s) that we want, and tidies the results for us. We could optionally further separate the last function into two functions, one for estimation and the other for tidying, rather than combining these steps. 

### Simulation Function 
### Wrong Way 3: Nested Loops

Often times you may have multiple parameter vectors to consider in your simulations--say, $v_1, v_2, v_3$. Then, some pseudo-code for your sims would nest loops over these parameter vectors, looking something like:

```{r, echo=T, eval=F}
for(i in v1){
  for(j in v2){
    for(k in v3){
      SIMULATE
    }
  }
}
```

This is a sub-optimal approach for a few reasons. First, the code is being run serially, at least in this example. Second, assuming you've managed to write nested, parallel loops, the usual methods for overcoming the seed-setting issue from point 2 are not currently supported for nested parallel loops. Thus, we run into reproducability problems again. Lastly, nested loops are generally less readable, and I assume we want easily interpretable code. Thus, I advocate a different approach here, although I will also include a bit on how to overcome the reproducibility issue for nested parallel loops, for those of you who require them for some reason. 

### Wrong Way 4: Replacing Matrix/Dataframe Elements

One of the more common approaches to simulations may be to create a dataframe or matrix with specified dimensions, and replace the elements of it with simulation results, like so:

```{r, echo=T, eval=F}
params = VECTOR OF SIM PARAMS

results = #
  matrix(
    NA,
    nrow = nsims,
    ncol = length(params)
  )

for(j in params and i in nsims){
  results[i,j] = SIMULATION OUTPUT
}
```

The issue with this approach is that it is extremely memory inefficient due to [R's copy-on-modify behaviors](https://adv-r.hadley.nz/names-values.html#copy-on-modify) (see [this](http://adv-r.had.co.nz/memory.html#modification) earlier, too). Although it is more efficient for dataframes than for matrices, the more efficient method is to store everything in a list, and then bind up the results later. 

### Wrong Way 5: Using Windows

If you are still using Windows to do data work, you are missing out on some clear efficiency gains. Most of all--and related to point 1--windows does not permit the use of forked processes for parallelism; rather, it relies on sockets. The [main difference](https://www.r-bloggers.com/parallel-r-socket-or-fork/) between these two is that forked processes share an environment, while socketed processes must communicate with the master process explicitly to get R objects. What this means is that socketed processes tend to have higher communication overhead, and therefore simulations will run slower than on a fork. 

Additionally, Windows tends to have a high number of background processes and bloatware, which all consume system resources. If you are on Windows and are interested in trying something new, I highly recommend [dual booting](https://itsfoss.com/guide-install-linux-mint-16-dual-boot-windows/) windows alongside a Linux distribution, which often has fewer processes, allows forking, has a very active user-base, and is completely free. For those of you who think that Linux is just for computer nerds, or perhaps are intimidated by the infamous terminal, I assure you that nowadays most Linux distributions have a very user-friendly GUI that is similar in its usage to Windows and Mac. If this is something you would like to give a try, I suggest either [Linux Mint Xfce, Linux Mint Cinnamon](https://linuxmint.com/download.php), or [MX Linux](https://mxlinux.org/), as these are all highly stable, fairly fast, and have very active communities. Note that MX Linux requires a fresh install of the OS every major release, while Mint does not. So, if you want to install just once, I'd start on Mint. You can always distro-hop later. 


