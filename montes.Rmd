---
title: "Monte Carlo Analysis in R: An in Depth Guide, with Examples"
date: "Updated: `r format(Sys.time(), '%d %b, %Y')`"
pagetitle: "Monte Carlos"
header-includes:
- \usepackage{mathtools}
- \usepackage{amsmath}
output:
  html_document:
    toc: yes
    number_sections: no
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: show
bibliography: bibliography.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---


```{r, echo=F, eval=T}
knitr::opts_chunk$set(class.output = "bg-success",
                      class.source = "fold-show",
                      tidy = F,
                      echo=T, 
                      eval=T,
                      message=F,
                      cache=T)

```

\


After quite a few iterations of writing Monte Carlo simulations, I've decided to compile a post on how to get started doing montes, all without having to endure some of the painful mistakes that I made. These mistakes include (but are not limited to):

1. Serial simulations/loops
2. Ignoring implicit parallelization
3. Improper seed-setting
4. Nested loops
5. Ignoring load balancing
6. Ignoring copy-on-modify overhead
7. Improper version control
8. Not using functions, or enough of them
9. Using Windows operating system

First, I will walk through some of the details, with toy examples to drive points home where applicable. Finally, I will take all of this and put it together in a more serious application where we explore the bias of logistic regression in small samples, as well as a penalized likelihood approach to solving this bias. 

*This guide will applies mostly to the case of trivial parallelism, i.e., simulations that are independent of each other. However some concepts can be extended to dependent simulations, particularly in MCMC algorithms that rely on full-conditionals. Where this is the case, I note it in the Caveats subsection of a topic. It should also be noted that this guide won't be useful for most extremely simple Monte Carlo problems, as parallelism can actually slow those types of tasks down.*

# Serial vs Parallel
***

If you're working in R, then the first step to better Monte Carlo analysis is implementing parallel simulations. There are exceptions to this rule (e.g., reserving computing resources for other tasks, parallel overhead making things slower), but if the reason you are not using parallel processing is simply because you do not know how, then I'll walk you through a few approaches here. 

For going parallel, there are essentially two options:

1. Using `foreach` loops, which are similar, yet distinct from the typical `for` loop

2. Using the parallelized `apply` family of functions, e.g., `parLapply`, `parApply`, and friends.

I've used both approaches extensively (in the past I only used the `apply` approach), and strongly prefer the `foreach` approach at this point for a few reasons:

  a. It tends to be more intuitive as its similar to `for` loops in appearance
  
  b. Tends to be easier to debug (in my opinion)
  
  c. Doesn't require nested function calls for complicated simulations
  
  d. Permits nested parallel looping
  
For these reasons, I won't be covering the `apply` family of functions here. I may eventually extend the principles and examples below to the `apply` family, but for now we'll focus on `foreach`.

\

### Differences between `foreach` and `for`
***

Since we'll be using `foreach`, its useful to compare and contrast it with a typical `for` loop. Suppose we just wanted to add 1 to each element in a vector, and for some reason we did this using a for loop in R (rather than leveraging vectorization). We might write the following:

```{r, echo=T, eval=T}
#for loop
vec = seq(1, 5)
out_for = vector()

for(i in seq_along(vec)) 
  out_for[i] = vec[i] + 1

print(out_for)
```

Then, using the `foreach` package, an equivalent loop could be written like so: 

```{r, echo=T, eval=T, message=F}
#foreach 
library(foreach)

out_foreach = 
  foreach(i = seq_along(vec),
          .combine = "c"
          ) %do% {
                        
    return(vec[i] + 1)
                        
  }

print(out_foreach)
```

Despite having a somewhat similar appearance to the `for` loop, `foreach` loops differ in two key ways:

1. They are functions. 

2. Because they are functions, they have a `return` value. 

In fact, `foreach` loops are composed of at least two functions. The first is a bit obvious, its a call to `foreach(...)`. The second is actually the [infix function](https://adv-r.hadley.nz/functions.html#function-forms), `%do%` or `%dopar%`. These functions are similar in concept to the matrix multiplication operator `%*%` in that they take arguments to the function on either side of the operator, and apply some set of instructions using these arguments. Similarly to the matrix multiplication operator, then, `%do%` takes arguments on either side. On the left-hand side is an object returned by the `foreach()` function call, and on the right is an `expression`, which for our purposes is just a bunch of text instructions for our loop. 

The second function call is the call to `foreach()`, which we just learned is on the left-hand side of an infix function call. Thus, its results are an argument to the `%do%` function. You can roughly think of `foreach()` as being the same as the serial case when we call `for(i in something)`: it is creating an object to be iterated over. In reality things are a bit more complicated than that, but the details are not too important here. It is enough to know that `foreach()` is a function, and it returns an object that is in some way useful for iterating and executing a loop in parallel. The results of calling this `foreach(...)` function are then used as the inputs on the left-hand side of `%do%`. 

Finally, on the right-hand side of `%do%` we have an `expression`-class object, which can just be thought of as our loop code. But, because these instructions will be executed on several CPUs simultaneously, we need to send them off to each core first, and _then_ execute the instructions and return their collective results. This is why `foreach` loops are actually function calls, because they have to send off a set of instructions in a unified way to several child processes for execution, keep track of that execution, combine the results, and return them in a sensible way. In contrast, `for` loops are running on a single process, so the instructions can just be executed on that process in real-time. 

If any of this is confusing, the main takeaway is that `foreach` loops are actually functions. Since functions have a `return` value, we stored the results in the `out_foreach` object, and gave the `foreach` loop an explicit `return()` call at the end. This is only a quick introduction to `foreach` though, so I highly recommend taking a look at its [vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html). 



\

### Making `foreach` Run in Parallel
***

Now that you have the basics of how `foreach` and `for` differ, we can move on to making things work in parallel. The above toy example is in fact running serially in both the `foreach` and `for` loops. However, to make the `foreach` loop run in parallel requires trivial alterations:

1. Tell R how many cores to use.

2. Register those cores as a parallel back-end using the `doParallel` package.

3. Change `%do%` to `%dopar%` (though later we'll see that a better alternative is actually `%dorng%`).

4. Shut down the parallel backend (**this is extremely important for PC longevity**).

```{r, echo=T, eval=T}
library(doParallel)

# 1. Use one fewer cores than max available due to using R studio
ncores = parallel::detectCores() - 1 

# 2. spin up the cluster
cl = makeCluster(ncores)
registerDoParallel(cl)

# 3. Change `%do%` to `%dopar%` 
out_foreach = foreach(i = seq_along(vec),
                      .combine = "c"
                      ) %dopar% {
                        
    return(vec[i] + 1)
  }

#4.shut down the cluster DONT FORGET THIS
stopCluster(cl)
```


\

# Seed Setting and Parallelism
***

If you're running simulations, you probably want to be able to reproduce them, either on your own machine at a later date, or on another machine. However, if you've now implemented parallel processing as suggested using `%dopar%`, **setting your seed is no longer as straightforward as in the serial case**. 

For example, lets simulate some random variables using both a serial loop and parallel loop, and set the seed before hand. We'd expect that the results are the same, but this turns out to be false. First, serially:

```{r, echo=T, eval=T}
# Serially
serial_1 = serial_2 = c()

set.seed(123)
for(i in 1:3) 
  serial_1[i] = rnorm(1)

set.seed(123)
for(i in 1:3) 
  serial_2[i] = rnorm(1)

identical(serial_1, serial_2)
```

which produces two vectors of the exact same numbers, as we'd expect given the above code. 

However, when we implement the "equivalent" in parallel (notice how the infix property becomes more apparent in the code below, too):

```{r, echo=T, eval=T, class.output = "bg-danger",}
# Parallel
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

set.seed(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

we end up with results which are not the same, despite setting the seed as one would typically do in the serial case. The details of why this is happening are technical and mostly unimportant; what is important is that there are some simple solutions.

\

### Option 1: `doRNG`
***

The simplest solution is to use the [`doRNG`](https://cran.r-project.org/web/packages/doRNG/vignettes/doRNG.pdf) package. There are two ways to use the package to reproduce simulation results. 

1. **`doRNG` Method 1**: Set your seed as usual, and replace `%dopar%` with `%dorng%`:

```{r, echo=T, eval=T, message=F}
# set.seed and  %dorng%
library(doRNG)
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dorng% { rnorm(1) }

set.seed(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dorng% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

\
2. **`doRNG` Method 2**: Register your back-end using `registerDoRNG()`, then use `%dopar%` as usual:

```{r, echo=T, eval=T, message=F}
# doRNG backend, %dopar%
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

registerDoRNG(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

registerDoRNG(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

Both approaches result in reproducible parallel loops. 

\

### Option 2: Core-level Seeds
***

A bit of a hacky alternative to `doRNG` is to simply set the seed on each core. Remember from earlier that the instructions set on the right-hand side of `%dopar%` are passed to each core. Well, if each core has a unique seed, then any time we run the loops, the same results will obtain. To do this we 

1. Set an "outer" seed in the traditional way. 

2. Sample "inner" seeds to be used in each set of parallel instructions. The number of inner seeds is equal to the number of simulations.

3. Set the seed within your simulation code using `set.seed()`, but index your inner seeds using the iteration number for the simulation.

For example:

```{r, eval=T, echo=T, results='hide'}
ncores = 3
nsims = 10
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)
inner_seeds = sample(
  x = 999999, 
  size = nsims
  )

parallel_1 = 
  foreach(i = 1:nsims, 
          .combine = c
          ) %dopar% {
            
    set.seed(inner_seeds[i])
    rnorm(1) 
    
  }

parallel_2 = 
  foreach(i = 1:nsims, 
          .combine = c
          ) %dopar% {
            
    set.seed(inner_seeds[i])
    rnorm(1) 
  }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

Now this is of course a bit hacky, but this approach does have practical utility. Namely, if you ever need nested parallel loops (using the `%:%` nesting operator detailed in the `foreach` documentation), then unfortunately the `%dorng%` backend is not supported. In this case, the only way I have found to successfully reproduce simulations is the above, core-level seed-setting approach. To be clear, this isn't setting the same exact seed on all cores. We made a vector of seeds, `inner_seeds`, and each simulation uses a seed from this vector before executing its instructions. You can think of this as setting a different seed across a bunch of different R sessions, and then combining the results; however, the universe of inner seeds remains the same, and therefore so does the results set across runs. 


\


# Thread Competition
***

Now we are running loops in parallel, and our pseudo-random numbers are reproducible as we would want. However, there is a potential lurking issue that will completely tank any simulations structured as we have here: other multithreaded/parallel tasks competing for CPU resources. In other words, if R is trying to execute simulations in parallel while also trying to execute some other task in parallel simultaneously, often times your computer will hang and eventually crash. 

So why would anyone be doing multiple forms of parallel processing simultaneously? The most common reason for this is when you are using optimized BLAS/LAPACK libraries while executing linear algebra-heavy simulations. That is, when you've installed some alternative, optimized linear algebra library and linked it up with R, such as [Open BLAS](https://www.openblas.net/), [MKL](https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html), or [Atlas BLAS](http://math-atlas.sourceforge.net/), and your simulations then utilize these libraries to execute matrix operations. In addition to having superior algorithms that are optimized for your specific hardware, these libraries often times have multithreaded matrix operations; ie, they do common matrix operations in parallel, such as matrix multiplication, inversion, etc. Thus, if you are using these libraries and forget to disable their inherent multithreading, they will compete with `foreach` when accessing your CPUs, which will ultimately hang your system. For example, if your computer has 8 threads, and you tell R to give `foreach` 7 of them, but MKL reserves 4 of them for matrix operations, suddenly you're trying to access 11 threads when you only have 8. The only option is to use the same thread for multiple tasks simultaneously, but this isn't possible so the computer hangs. 

\

### Checking BLAS/LAPACK Threading
***

To check if your PC is running multi-threaded matrix operations, use the [`RhpcBLASctl`](https://cran.r-project.org/web/packages/RhpcBLASctl/index.html) package:
```{r, echo=T, eval=T}
library(RhpcBLASctl)
blas_get_num_procs()
omp_get_num_procs()
```

If either of these results is greater than 1, then you have multithreading enabled. We won't delve into the differences between BLAS and OMP here; suffice it so say that you want both of these at 1 for single-threaded matrix operations. <span style="color: red;">**IF YOU USE [MICROSOFT'S R-OPEN](https://mran.microsoft.com/open)**, **_then you definitely have MKL enabled as your BLAS/LAPACK library, and multi-threading is likely enabled by default_**</span>. 

Alternatively, you can check on the name of your BLAS library, which will tell you at least if its not the default library:

```{r, echo=T, eval=T}
sessionInfo()[c("BLAS", "LAPACK")]
```

but this isn't conclusive, as it merely tells us that I'm using MKL, not that MKL is currently set to use multiple threads. For that reason, the `RhpcBLASctl` method is probably superior.

\

### Multi-threaded Packages
***

In addition to multi-threaded matrix operations, there are other R packages that are multithreaded by default and will therefore compete for CPU resources. The number one culprit here is probably [`data.table`](https://cran.r-project.org/web/packages/data.table/index.html). However, other machine learning libraries have the option for multithreading as well, such as [`caret`](https://topepo.github.io/caret/), and [`glmnet`](https://cran.r-project.org/web/packages/glmnet/index.html). To my knowledge neither of these last two are multithreaded by default, so this only becomes a problem if you accidentally tell them to use more than one CPU. 

\
  
### Solving Thread Competition
***

Fortunately, it is extremely easy to solve thread competition.

1. To disable multithreaded matrix operations, use the [`RhpcBLASctl`](https://cran.r-project.org/web/packages/RhpcBLASctl/index.html) package, **and be sure to use the code within your parallel loops so that each child process disables the multi-threading.**

    ```{r, echo=T, eval=T, results='hide'}
# disable in parent process 
library(RhpcBLASctl)
library(doRNG)

blas_set_num_threads(1)
omp_set_num_threads(1)

ncores = 3 
nsims = 3
cl = makeCluster(ncores)
registerDoParallel(cl)
set.seed(123)

# disable in child processes 
  foreach(i = 1:nsims,
          .packages = "RhpcBLASctl"
          ) %dorng% {
            
    library(RhpcBLASctl)
    blas_set_num_threads(1)
    omp_set_num_threads(1)   
    
    rnorm(1) 
    
          }
  
stopCluster(cl)
    ```
<br>

2. To disable packaged multithreading, you'll just have to read the documentation for that package to determine how to set the max number of threads to 1. For example, browsing the documentation for `data.table` immediately tells us that the `setDTthreads()` function does what we want. 

\

### Caveats
***

There are a few qualifiers worth mentioning here. 

1. If you aren't using matrix operations in your parallel tasks, then multi-threaded matrix operations shouldn't cause problems in theory, because they aren't even being attempted. However, if they aren't happening, it also doesn't hurt to just set the number of threads to 1. Just make sure the change it back if you want to leverage that efficiency in your R session later on. 

2. If you aren't using the multi-threaded R packages, perhaps it goes without saying that you don't need to worry about them competing for resources in this way.

3. It is possible that some combination of matrix or package multithreading along with parallel simulations is "optimal," in the sense that its faster. For example, if you have 16 threads you could assign 5 threads to `foreach`, and then have each of those processes use 2 threads for matrix operations. You'd then have 15 threads in use, plus one for R-studio or other tasks on reserve. Perhaps this would be faster overall than single-threaded matrix operations on 15 simulation processes. I haven't done any testing for this, so I have no idea whether or when this would be better. At the extreme, it could in theory be better to use a single threaded simulation (i.e., serial simulations), while using all cores for matrix operations. I imagine that is an edge case, but again I cannot rule it out as I have not investigated this extensively. In any case, I'm lazy, so I just disable multi-threading on all packages and matrix operations, and give all my threads to `foreach`. 

\

# Parameter Grids 
***

Now that we're working in parallel without suffering thread competition and can consistently reproduce our results, the next step is to create a grid of parameters for our simulations, i.e., the points in the parameter space at which we'll investigate the behavior of our chosen procedure. These will of course be specific to the process under study, but its worth showing a general way to go about creating this grid. 

As a contrived toy example, suppose our task involves normal random variables with different means and variances. Specifically, we are interested in $X \sim \mathcal{N}\left(\mu \in \{0, -1 \}, \,\,\sigma^2 \in \{1, 3\}\right)$; that is, we'll vary the mean and variance of some random variable $X$. Additionally, we want to investigate how sample size affects the properties of these random draws. Right now its not important *why* we're doing this or what $X$ will be used for, only that we want random variables with every pairwise combination of these parameter values. 

To do so, we create vectors of these parameters, and then use `expand.grid()` to get the pairwise combinations:

```{r}
mu = c(0, -1)
sig2 = c(1, 3)
n = c(5, 10)

(grid = expand.grid(mu = mu, sig2 = sig2, n = n))
```


So now we have the parameters we need for our simulations in a nice tabular format, which we can then loop over when creating data from our chosen data generating process. For example (omitting the back-end setup, etc. for clarity):

```{r, eval=F}
# SETUP OMITTED FOR SIMPLICITY. SEE EXAMPLES SECTION FOR FULL SETUP. 

out = foreach(i = 1:nrow(grid)
        ) %dopar% {
            
    
    rnorm( n = grid[i, "n"],
           mean = grid[i, "mu"],
           sd = sqrt(grid[i, "sig2"])
    )
    
  }
```

Right now, the output from this is not the focus. The main point here is that we've created a `grid` of all unique pairwise parameter combinations, and we've looped over each row of that `grid` to then get random numbers using every possible parameter combination. In this way we've ensured that we simulate data from each of the desired parameterizations, all without having to manually specify the parameters, and without having to do any sort of nested looping, e.g. `for(i in mu){ for j in sig2 {...}}`. This is akin to a grid-search in optimization, although here we're not searching for an optimal set of parameters, we're investigating the behavior of some procedure at pre-defined grid points in the parameter space. 

\

### Extending Grids to Multiple Simulations
***

Of course, *we've only done one simulation* here. That is, at each set of parameter values, we have one iteration. But in Monte Carlo analysis, we want some large number of simulations in order to see how things are behaving, so we'll need to replicate this grid. Suppose we want 2 simulations instead of 1, then we can use `do.call()`, `cbind.data.frame`, and `replicate()` together to get a grid with each grid point appearing twice, so that we'll have two simulations at that point instead of one:

```{r}
nsims = 2 

mu = c(0, -1)
sig2 = c(1, 3)
n = c(5, 10)

grid = expand.grid(mu = mu, sig2 = sig2, n = n)
(grid_multi = do.call(rbind.data.frame, replicate(nsims, grid, simplify=F)))
```

And we can now see that each grid point appears twice, so if we were to loop over this in the same as as we did previously, we produce results for each parameter combination twice. Inductively then, we know that for arbitrary values of `nsims`, we'll get `nsims`-many results for each parameter combination, which is precisely what we want in Monte Carlo analysis. 

Briefly explaining the above code, `replicate()` is replicating the `grid` dataframe `nsims`-many times, and storing these replicates in a list. Then, `do.call` calls the `rbind.data.frame` function with each element of the replicated list as its arguments. Mathematically, `do.call` is effectively executing $f(x_1, x_2, ..., x_{nsims})$ where $f()$ is `rbind.data.frame`, and each `x_j` is an element of a list. In this case, the list-elements are all the original, single-simulation `grid` object.  

\

### Why not use a nested loop? 
***

One potential question is why not use a nested loop instead of replicating each grid point `nsims`-many times. That is, we could just repeatedly loop over the original `grid` object, rather than replicating it several times, like so (omitting the back-end setup, etc. for clarity): 

```{r, echo=T, eval=F}
# SETUP OMITTED FOR CLARITY. SEE ENDING EXAMPLES FOR FULL SETUP.

# Make grid WITHOUT REPLICATING 
mu = c(0, -1)
sig2 = c(1, 1.5)
n = c(5, 10)

grid = expand.grid(mu = mu, sig2 = sig2)

# make number of simulations a variable for looping
nsims = 100

out = foreach(i = 1:nrow(grid)
        ) %dorng% {
            
    res = list()
    
    # create an inner, nested loop that repeatedly simulates from a gridpoint
    # instead of replicating the gridpoint and looping over the replicates
    for(j in seq_len(nsims)){

      res[[j]] = rnorm( n = grid[i, "n"],
                      mean = grid[i, "mu"],
                      sd = sqrt(grid[i, "sig2"])
              )
    }
    
    return(res)
    
}
```

The key difference here is that we create a single set of grid points without duplication via `replicate`, and then create an inner loop (`for(j in seq_len(nsims))`) that repeatedly uses these grid points `nsims`-many times. An obvious advantage of the nested-loop approach is that it conserves memory; the grid is not replicated, so each core only stores a copy of the original, single-simulation grid.  The memory gains are somewhat trivial though due to how R stores lists in memory, but that is a technical aside that I won't delve into now (see [this page](https://adv-r.hadley.nz/vectors-chap.html#lists) for more info). 


However, the major disadvantage of the nested-loop approach is load balancing. In most real-world applications we will be varying the sample size of our procedure, and by much more than we are doing so here; i.e., we'll estimate some model at small and large sample sizes. In doing so, the nested-loop approach becomes problematic for load balancing purposes, because the CPUs that have the small sample sizes will take way less time to complete their inner, nested loop (recall, the set of instructions on the right-hand side of `%dopar%` are executed on a single CPU). Eventually then, the CPUs with the large sample sizes will still be running their nested, inner-loops while the CPUs that had the smaller sample sizes have completed their tasks and are *sitting idle* doing nothing. That all in mind, for the specific toy examples here it turns out that the nested approach is faster than the replicating approach due to parallel processing overhead being greater than the time it takes to complete a task (see [here](https://en.wikipedia.org/wiki/Parallel_slowdown) for more on this). However, for more interesting cases, this will generally not be true.

\

### Caveats 
***

In most cases, the replicated-grid approach will be better, often times by several hours when compared with the parallel-outer, serial-inner approach. However, there are at least two instances where this won't hold, one of which was alluded to in the previous paragraph. 


1. If the run-time of a single simulation is small, e.g., < 1 second, it is possible that the overhead from parallelizing at the simulation level---rather than the grid-point level---will exceed the gains from parallelizing. This is known as parallel slow down. Its possible to mitigate this by using prescheduling, but even this won't always fix the issue if single simulations take a very small amount of time.  Prescheduling splits the total number of tasks into chunks (at random) and assigns these tasks to each CPU. So if you allocate 10 CPUs, and there are 100 tasks, the computer will randomly select 10 tasks to assign to each CPU. Without prescheduling, when a CPU finishes a task, it will communicate with the other CPUs in the network or the parent process and determine what tasks still need doing, and then it does one of the ones that remains. So, by prescheduling we reduce some of this communication overhead. There are advantages and disadvantages to prescheduling that I won't go into here, but a general rule is if your tasks are numerous and require little time individually, use prescheduling, otherwise don't. To enable presheduling in `foreach`, do the following:

    ```{r, eval=F}

    foreach(i = 1:nsims, 
        .options.multicore = list(preschedule = TRUE)) 
        %dorng% {
          
          [...code...]
        
        }
    ```

 

2. If you are doing some form of Markov Chain Monte Carlo (MCMC), rather than traditional Monte Carlo analysis, the nested loop approach will generally become **_necessary_**. For things like the Metropolis-Hastings algorithm and its derivatives (e.g., Metropolis, Gibbs), we are typically sampling from a full-conditional distribution that depends on some previous iteration. So parallelization here would occur at the chain-level, and within each chain we would have serial loops to sample from the full conditionals. The gridding approach would also not make as much sense anyway, as the proposal parameters are not fixed ex-ante aside from the starting values.

\

# Using Functions Properly 

***

At this point we've set up our environment, and are ready to start with the actual simulations. To recap, we are using `foreach` combined with `%dopar%` or `%dorng%` for parallelism, we've ensured reproducability, we have handled thread competition with other parallel processes, and now we have a grid of parameter values at which to investigate the behavior of some procedure, with each grid value appearing once for each simulation iteration. Now we can make some functions that will automate the simulation and transformation of our data.

R is an object oriented, functional programming language. We should leverage both of these facts to make our simulations easier to understand, implement, and debug. At a minimum, this will consist of having two functions: 

1. A **simulation function** that creates the data to be further analyzed. 

2. An **estimation/analysis function** that estimates the model(s) that we want, and tidies the results for us. 

We could optionally further separate the estimation function (2) into two functions, one for estimation and the other for tidying, rather than combining these steps. 

This approach to Monte Carlo analysis---having a simulation function, and an estimation function---is one of the most important reasons why I choose not to use the parallelized `apply` family of functions. If you're unfamiliar with the `apply` family of functions, you can either skip this paragraph or [familiarize yourself](https://www.r-bloggers.com/2015/07/r-tutorial-on-the-apply-family-of-functions/) with them. Because our data must be both simulated and then used in estimation, the `apply` family of functions requires that there be either (i) a single function that does both of these tasks (e.g., via nesting), or (ii) that we pre-simulate every single dataset that will be needed for the estimation procedure, and then call the estimation function on each of them using `apply` functionality. The first approach is bad because debugging functions becomes more nightmarish as they get more complicated, so smooshing two functions into one is ill-advised. The second approach is unfeasible in most interesting cases because there is usually not enough memory to store hundreds of thousands of pre-simulated datasets. Thus, because we'll compartmentalize the simulation and estimation into two functions, the `foreach` approach becomes the natural one to implement. 

\

### Simulation Function 

The goal of the simulation function is to take in one row from our parameter grid, and use those parameters to create a dataset that will eventually be transformed via estimation or some other procedure of interest. Keeping with the toy example used previously, lets suppose we are just simulating some realizations of a normal random variable at various grid points and sample sizes. We have our grid from earlier: 

```{r}
print(grid_multi)
```

which has 3 parameters, `mu`, `sig2` and `n`. So our simulation function will have to take in *at least* these parameters in some way, and potentially others. It is my view that the easiest way to do this is just to take in a vector of parameters as a single parameter to the function, rather than explicitly naming each parameter to the function. This way, if the grid columns change, we don't have to then go and add or remove parameters from the simulation function. 

For example, we could specify the parameters explicitly, then index our grid by row when passing the arguments (in this case, just row 1): 

```{r}
simulate_fn = function(mu, sig2, n){
  
  return(
    list(
      mu = mu, 
      sig2 = sig2, 
      n = n
    )
  )
  
}

simulate_fn(
  mu = grid_multi[1, "mu"], 
  sig2 = grid_multi[1, "sig2"],
  n = grid_multi[1, "n"]
  )
```

But if we later decide we want to add new parameter to the grid, we then have to update the `simulate_fn` function accordingly. Updating the function's parameters can be avoided if we just pass a named parameter vector as a single parameter, and unpack it inside the function:

```{r}
simulate_fn = function(grid_row){
  
  grid_row = unlist(grid_row) # re-class to vector from data.frame

  return(
    list(
      mu = grid_row["mu"], 
      sig2 = grid_row["sig2"], 
      n = grid_row["n"]
    )
  )
  
}

simulate_fn(grid_multi[1,])
```

Using this second approach then, we can define a function to simulate the normal realizations, and see what it produces for a single iteration on our grid: 

```{r}
simulate_fn = function(grid_row){
  
  grid_row = unlist(grid_row)
  
  rnorm(
    n = grid_row["n"], 
    mean = grid_row["mu"],
    sd = sqrt(grid_row["sig2"])
  )
  
}

simulate_fn(grid_row = grid_multi[1,])
```
which is a numeric vector, as we would expect. Now we can do whatever transformation of this data to obtain statistics of interest for each iteration. However, we'll need an estimation function first. 

\ 

### Estimation Function 
***
To keep up the toy example, lets say now that we're interested in the estimators of the first and second moments of a normal random variable after we've exponentiated it (that is, of the log-normal distribution). So we'll need a function that takes in the data, exponentiates it, and computes these estimates. **Importantly,** we want to know how the various parameters under consideration impact these estimators (if at all), so we need to also keep track of the parameters that we're using. One possibility is the following, where we again pass in the data-generating grid row:

```{r}

estimate_fn = function(dat, grid_row){
  
  dat_exp = exp(dat)
  xbar = mean(dat_exp)
  s2   = var(dat_exp)
  
  out = c(xbar = xbar, s2 = s2, grid_row)
  
  return(out)
}
```

So now we'll simulate some data using `simulate_fn` and return the quantities of interest from this data using the `estimate_fn`:

```{r}
grid_row = unlist(grid_multi[1,])

dat = simulate_fn(grid_row = grid_row)

estimate_fn(
  dat = dat, 
  grid_row = grid_row
)
```
which gives us the sample mean and variance for a single iteration of transforming data to log-normal realizations, as well as stores the data-generating parameters for use in post-simulation analysis. 

\ 

# Putting it All Together
*** 

Before we jump into some more serious examples, we can combine everything we've learned using the toy example, just to give an idea about the structure. I'm not going to separate the code out into each step below; all of it will appear in a single chunk with comments so as to mimic what we would actually have in a script. I'll also be doing things in a slightly different order than was presented in the guide. This is again for the example of analyzing the estimators of the first two moments of the log-normal distribution under various parameterizations. I'll do 30 simulations at each grid point for demonstration purposes, but in reality you'd probably want more. 

```{r}
# Load up packages 
pks = c("doParallel", "doRNG", "RhpcBLASctl")
needed = setdiff(pks, rownames(installed.packages()))

if(length(needed) > 1)
  install.packages(pks, Ncpus = parallel::detectCores() - 1)

invisible(sapply(pks, library, character.only=T))

# Define Simulation and Estimation Functions
simulate_fn = function(grid_row){
  
  rnorm(
    n = grid_row["n"], 
    mean = grid_row["mu"],
    sd = sqrt(grid_row["sig2"])
  )
  
}

estimate_fn = function(dat, grid_row){
  
  dat_exp = exp(dat)
  xbar = mean(dat_exp)
  s2   = var(dat_exp)
  
  out = c(xbar = xbar, s2 = s2, grid_row)
  
  return(out)
}

# Make the parameter grid 
nsims = 300 

mu = c(0, -1)
sig2 = c(1, 3)
n = c(5, 10)

grid = expand.grid(mu = mu, sig2 = sig2, n = n)
grid_multi = do.call(rbind.data.frame, replicate(nsims, grid, simplify=F))

# Disable BLAS/LAPACK multithreading in parent process 
blas_set_num_threads(1)
omp_set_num_threads(1)

# Spin up backend
nprocs = parallel::detectCores() - 1
os = .Platform$OS.type
type = if(grepl('unix', os)) 'FORK' else 'PSOCK'
cl = makeCluster(nprocs, type = type)
registerDoParallel(cl)

# Run sims 
set.seed(123)

res = foreach(i = seq_len(nrow(grid_multi)),
              .packages = 'RhpcBLASctl',
              .errorhandling = "pass",
              .inorder = F,
              .options.multicore = list(preschedule=T)
              ) %dorng% {
    
  blas_set_num_threads(1)
  omp_set_num_threads(1)
  
  grid_row = unlist(grid_multi[i,])
    
  dat = simulate_fn(grid_row)
  out = estimate_fn(dat, grid_row)
  
  return(out)
  
}

stopCluster(cl)
```


#  Using Windows

If you are still using Windows to do data work, you are missing out on some clear efficiency gains. Most of all--and related to point 1--windows does not permit the use of forked processes for parallelism; rather, it relies on sockets. The [main difference](https://www.r-bloggers.com/parallel-r-socket-or-fork/) between these two is that forked processes share an environment, while socketed processes must communicate with the master process explicitly to get R objects. What this means is that socketed processes tend to have higher communication overhead, and therefore simulations will run slower than on a fork. 

Additionally, Windows tends to have a high number of background processes and bloatware, which all consume system resources. If you are on Windows and are interested in trying something new, I highly recommend [dual booting](https://itsfoss.com/guide-install-linux-mint-16-dual-boot-windows/) windows alongside a Linux distribution, which often has fewer processes, allows forking, has a very active user-base, and is completely free. For those of you who think that Linux is just for computer nerds, or perhaps are intimidated by the infamous terminal, I assure you that nowadays most Linux distributions have a very user-friendly GUI that is similar in its usage to Windows and Mac. If this is something you would like to give a try, I suggest either [Linux Mint Xfce, Linux Mint Cinnamon](https://linuxmint.com/download.php), or [MX Linux](https://mxlinux.org/), as these are all highly stable, fairly fast, and have very active communities. Note that MX Linux requires a fresh install of the OS every major release, while Mint does not. So, if you want to install just once, I'd start on Mint. You can always distro-hop later. 


