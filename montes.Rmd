---
title: "Monte Carlo Analysis in R: An in Depth Guide, with Examples"
date: "Updated: `r format(Sys.time(), '%d %b, %Y')`"
pagetitle: "Monte Carlos"
header-includes:
- \usepackage{mathtools}
- \usepackage{amsmath}
output:
  html_document:
    toc: yes
    number_sections: no
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: show
bibliography: bibliography.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---

\newcommand{\E}{\mathbb{E}}

<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: SlateGrey;
}
</style>


```{r, echo=F, eval=T}
knitr::opts_chunk$set(class.output = "bg-success",
                      class.source = "fold-show",
                      tidy = F,
                      echo=T, 
                      eval=T,
                      message=F,
                      cache=T)

```

\

In this guide I discuss in-depth five steps to setting up parallel Monte Carlo analyses using R. Generally, these steps consist of:


1. [Switching from serial to parallel loops](#serial-vs-parallel)
2. [Ensuring reproducibility by setting seeds properly for parallel tasks](#seed-setting-and-parallelism)
3. [Ensuring that other parallel tasks are not competing for CPU resources](#thread-competition)
4. [Properly setting up simulation parameter grids](#parameter-grids)
5. [Compartmentalizing code into simulation and estimation functions](#using-functions-properly)


First, I will walk through each of these five points with toy examples to drive ideas home where applicable. Finally, I will take all of this and put it together in some more serious applications showing the downward bias of autoregressive parameters in time series models, small sample bias of logistic regression parameters (as well as a correction), and the inefficiency of the Heckman-correction in selection modeling. 

Before getting started though, there are **a few items to take note of**: 

* I assume that you are familiar with the basics of the [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method). I won't be covering its principles at all here. If you are unfamiliar with the method, it would be wise to read up on it before reading through this guide, as most of this won't make much sense otherwise. An in-depth book on the topic is available (for purchase) [here](https://onlinelibrary.wiley.com/doi/book/10.1002/9781118014967), but there are lots of free online resources available elsewhere.

* This guide will applies mostly to the case of trivial parallelism, i.e., simulations that are independent of each other. However some concepts can be extended to dependent simulations, particularly in MCMC algorithms that rely on full-conditionals. Where this is the case, I note it in the Caveats subsection of a topic. 

* This guide won't be as useful for most extremely simple Monte Carlo problems because I emphasize doing things in parallel, but parallel processing can actually slow down simple Monte Carlos relative to a serial processing. Some of the principles in this guide apply regardless, but all examples and discussion assume the parallel case. 

\

# Serial vs Parallel
***

If you're working in R, then the first step to better Monte Carlo analysis is implementing parallel simulations. There are exceptions to this rule (e.g., reserving computing resources for other tasks, parallel overhead making things slower), but if the reason you are not using parallel processing is simply because you do not know how, then I'll walk you through a few approaches here. 

For going parallel, there are essentially two options:

1. Using `foreach` loops, which are similar, yet distinct from the typical `for` loop

2. Using the parallelized `apply` family of functions, e.g., `parLapply`, `parApply`, and friends.

I've used both approaches extensively (in the past I only used the `apply` approach), and strongly prefer the `foreach` approach at this point for a few reasons:

  a. It tends to be more intuitive as its similar to `for` loops in appearance
  
  b. Tends to be easier to debug (in my opinion)
  
  c. Doesn't require nested function calls for complicated simulations
  
  d. Permits nested parallel looping
  
For these reasons, I won't be covering the `apply` family of functions here. I may eventually extend the principles and examples below to the `apply` family, but for now we'll focus on `foreach`. This is only a quick introduction to `foreach` though, so I highly recommend taking a look at its [vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html). If you're more of a DataCamp or otherwise type of person, there is an entire course on [parallel processing in R](https://www.datacamp.com/courses/parallel-programming-in-r) available through them (I am not affiliated with DataCamp, nor do I receive anything from them for linking this). There are also more in-depth resources on parallel processing in R available [here](https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html), [here](https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html), and [here](https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html), although Googling around will definitely yield droves of additional content.


\

### Differences between `foreach` and `for`
***

To see how `foreach` works, its useful to compare and contrast it with a typical `for` loop. Suppose we just wanted to add 1 to each element in a vector, and for some reason we did this using a for loop in R (rather than leveraging vectorization). We might write the following:

```{r, echo=T, eval=T}
# for loop
vec = seq(1, 5)
out_for = vector()

for(i in seq_along(vec)) 
  out_for[i] = vec[i] + 1

print(out_for)
```

\

Then, using the `foreach` package, an equivalent loop could be written like so: 

```{r, echo=T, eval=T, message=F}
# foreach 
library(foreach)

vec = seq(1, 5)

out_foreach = 
  foreach(i = seq_along(vec),
          .combine = "c"
          ) %do% {
                        
    return(vec[i] + 1)
                        
  }

print(out_foreach)
```

Despite having a somewhat similar appearance to the `for` loop, `foreach` loops differ in two key ways:

1. They are functions. 

2. Because they are functions, they have a `return` value. 

In fact, `foreach` loops are composed of at least two functions. The first is a bit obvious, its a call to `foreach(...)`. The second is actually the [infix function](https://adv-r.hadley.nz/functions.html#function-forms), `%do%`, or as we'll see later, `%dopar%` or `%dorng%`. 

Starting with the `%do%` infix function, notice its similar appearance to the matrix multiplication operator, `%*%`. Just as the `%*%` matrix multiplication operator takes its arguments on either side, so does the `%do%` or `%dopar%` operator (or any other arbitrary infix function). Thus, we can conclude that **the call to `foreach(...)` on the left-hand side, and the set of loop instructions on the right-hand side between the curly braces `{...}` are both arguments to the `%do%` function**. 

Moving to the second function, the call to `foreach(...)` on the left-hand side of `%do%`, we've just learned that this function (or rather, the _return value_ of this function) is an argument to the `%do%` operator. Its reasonable to ask what the `foreach()` function returns, then. Lets take a look:

```{r}
fe_call = foreach(i = 1:3)

str(fe_call)
```

So `foreach()` returns a list, with a whole bunch of named elements. We'll touch on some of these later on, but for now you can roughly think of `foreach()` as being the same as the serial case when we call `for(i in something)`: it is creating an object to be iterated over. In reality things are a bit more complicated than that, but the details are not too important here. It is enough to know that `foreach()` is a function, and it returns an object that is in some way useful for iterating and executing a loop in parallel. The results of calling this `foreach(...)` function are then used as the inputs on the left-hand side of `%do%`. 

Finally, on the right-hand side of `%do%` we have an `expression`-class object, which can just be thought of as our loop code. But, because these instructions will be executed on several CPUs simultaneously, we need to send them off to each core first, and _then_ execute the instructions and return their collective results. This is why `foreach` loops are actually function calls, because they have to send off a set of instructions in a unified way to several child processes for execution, keep track of that execution, combine the results, and return them in a sensible way. In contrast, `for` loops are running on a single process, so the instructions can just be executed on that process in real-time. 

If any of this is confusing, the main takeaway is that `foreach` loops are actually functions. Since functions have a `return` value, we stored the results in the `out_foreach` object, and gave the `foreach` loop an explicit `return()` call at the end. 


\

### Making `foreach` Run in Parallel
***

Now that you have the basics of how `foreach` and `for` differ, we can move on to making things work in parallel. The above toy example is in fact running serially in both the `foreach` and `for` loops. However, to make the `foreach` loop run in parallel requires trivial alterations:

1. Tell R how many cores to use.

2. Register those cores as a parallel back-end using the `doParallel` package.

3. Change `%do%` to `%dopar%` (though later we'll see that a better alternative is actually `%dorng%`).

4. Shut down the parallel backend (**this is extremely important for PC longevity**).

```{r, echo=T, eval=T}
library(doParallel)

# 1. Use one fewer cores than max available due to using R studio
ncores = parallel::detectCores() - 1 

# 2. spin up the cluster
cl = makeCluster(ncores)
registerDoParallel(cl)

# 3. Change `%do%` to `%dopar%` 
out_foreach = foreach(i = seq_along(vec),
                      .combine = "c"
                      ) %dopar% {
                        
    return(vec[i] + 1)
  }

#4.shut down the cluster DONT FORGET THIS
stopCluster(cl)
```

\

### Important `foreach` Parameters
***

Although this is certainly not a guide on the `foreach` package (see the [previous links](#serial-vs-parallel) for more detail on the package), there are a few parameters worth discussing here. 

* `.combine` --- This is a function that tells `foreach` how to combine the results, e.g., `list`, `c` (for concatenate), `rbind`, etc. (or some custom function). I typically leave this `NULL`, which returns the results in a list. I do this because combining results takes time, and I don't necessarily want to take that time immediately after simulations complete. 

* `.inorder` --- A logical value that indicates whether the results should be combined in the order they were submitted to the workers. I always set this to `FALSE` because I don't care about the ordering, I just want my results as fast as possible. **NOTE: the default is `TRUE`, meaning you combine in order by default.**

* `.multicombine` --- Logical value indicating whether the `.combine` function can take more than 2 arguments. For most conventional functions, e.g., `list`, `cbind`, `rbind`, `c`, this should be `TRUE`. This matters because if the `.combine` function has more than 2 arguments, then the `do.call` function can be used to call it on a list, which will tend to be faster than running a loop to combine results iteratively. Essentially, be sure to set this to `TRUE` if using the conventional functions listed here. 

* `.maxcombine` --- This tells `foreach` how many sets of results to combine at one time. The default is 100 results at a time, which is going to be painfully slow for any modest set of Monte Carlo results. I tend to set this to the total number of expected results, meaning that `foreach` should combine them all at once. With modern hardware this shouldn't be too problematic, but I stand to be corrected. 

* `.errorhandling` --- Tells how to handle errors on each thread. When I'm debugging, I typically set this to `"stop"`, so that errors kill the simulations. However, once I know there are no issues, I set this to `"pass"` so that errors are ignored, but their message is kept in the results. For complicated problems this is useful because there are instances where an algorithm may not converge due to random chance in the data generating process. But, we don't want to kill all simulations when that happens, we want to just keep going but know that for that particular iteration the algorithm did not converge. 

* `.packages` --- This tells `foreach` what packages to export to each parallel process. Remember, `foreach` spins up a separate instance of R on each core, so we have to tell these cores what packages to load. We do so using the `.packages` parameter. 

* `.export` --- This tells `foreach` which objects from the global environment to export to each process. Again, because separate instances of R are created, we need to ensure that each process has access to all the relevant objects. **Note:** This is less relevant when using a `FORK` cluster, as detailed in the next section. 

\

### `FORK` or `PSOCK`?
***

When spinning up a parallel cluster, e.g., using `makeCluster(cl)` as we did [previously](#making-foreach-run-in-parallel), there is a `type` parameter that controls the type of cluster that we create, which is one of either `"FORK"` or `"PSOCK"`. The default is `"PSOCK"`, but this can be set, e.g. by calling `makeCluster(your_cluster_object, type = "FORK")`. The details on each cluster type are as follows:

* `"FORK"` --- This type of cluster duplicates the master process on each child process, but allows them all to access the same shared global environment from the master process. Think of the master process as the process that you are interacting with in R where you type your code. So, multiple R processes are spawned on your cores, but they all access the same single global environment. This minimizes parallel overhead communication between the processes. **NOTE:** This type of cluster is only available on UNIX-alike machines (i.e., GNU-Linux, Mac), so <span style="color: red;">**Windows users cannot use this cluster type**</span>.

* `"PSOCK"` --- This type of cluster does not have a shared environment, new global environments are created for each of the child processes. So, we have to tell the master process which objects to export to its children using the `.export` option as mentioned in the previous section, because entirely new (and empty) environments are created for each child process. 

Because of the copying and ensuing communication required for `"PSOCK"` clusters, some estimates put `"FORKS"` as being [roughly 40% faster than socket clusters](https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/). Again though, if you're using Windows, you won't be able to leverage this forking speed bonus. 

If you would like to be able to fork but are on Windows, then [dual-booting](https://opensource.com/article/18/5/dual-boot-linux) a Linux OS alongside your Windows installation is a free way to do so while keeping your Windows installation intact. Its also a great way to start learning about the Linux system, as this is often the system of choice in many industrial data science positions. The purpose of this guide is not to give instruction on how to do this, but a good starting Linux distribution is [Linux Mint Cinnamon Edition](https://linuxmint.com/edition.php?id=292), which is built on top of the popular Debian-based [Ubuntu](https://ubuntu.com/). If this interests you, I suggest reading up on dual-booting Windows with Linux, and starting with either of the two distributions listed here. After some more experience with Linux, the Arch-based [Manjaro KDE](https://manjaro.org/downloads/official/kde/) is great, and is in fact what I use daily. Finally note that most popular Linux distributions are like any other operating system in that they have a graphical user interface and can be completely navigated using point-and-click actions, in case you mistakenly think that using Linux implies knowing a bunch about computer science. 

\

# Seed Setting and Parallelism
***

If you're running simulations, you probably want to be able to reproduce them, either on your own machine at a later date, or on another machine. However, if you've now implemented parallel processing as suggested using `%dopar%`, **setting your seed is no longer as straightforward as in the serial case**. 

For example, lets simulate some random variables using both a serial loop and parallel loop, and set the seed before hand. We'd expect that the results are the same, but this turns out to be false. First, serially:

```{r, echo=T, eval=T}
# Serially
serial_1 = serial_2 = c()

set.seed(123)
for(i in 1:3) 
  serial_1[i] = rnorm(1)

set.seed(123)
for(i in 1:3) 
  serial_2[i] = rnorm(1)

identical(serial_1, serial_2)
```

which produces two vectors of the exact same numbers, as we'd expect given the above code. 

However, when we implement the "equivalent" in parallel (notice how the infix property becomes more apparent in the code below, too):

```{r, echo=T, eval=T, class.output = "bg-danger",}
# Parallel
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

set.seed(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

we end up with results which are not the same, despite setting the seed as one would typically do in the serial case. The details of why this is happening are technical and mostly unimportant; what is important is that there are some simple solutions.

\

### Option 1: `doRNG`
***

The simplest solution is to use the [`doRNG`](https://cran.r-project.org/web/packages/doRNG/vignettes/doRNG.pdf) package. There are two ways to use the package to reproduce simulation results. 

1. **`doRNG` Method 1**: Set your seed as usual, and replace `%dopar%` with `%dorng%`:

```{r, echo=T, eval=T, message=F}
# set.seed and  %dorng%
library(doRNG)
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dorng% { rnorm(1) }

set.seed(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dorng% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

\
2. **`doRNG` Method 2**: Set your seed using `registerDoRNG()` instead, then use `%dopar%` as usual:

```{r, echo=T, eval=T, message=F}
# doRNG backend, %dopar%
ncores = 3
cl = makeCluster(ncores)
registerDoParallel(cl)

registerDoRNG(123)
parallel_1 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

registerDoRNG(123)
parallel_2 = 
  foreach(i = 1:3, .combine = c) %dopar% { rnorm(1) }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

Both approaches result in reproducible parallel loops. 

\

### Option 2: Iteration-level Seeds
***

A bit of a hacky alternative to `doRNG` is to simply set the seed for each iteration. Remember from earlier that the instructions set on the right-hand side of `%dopar%` are passed to each core. Well, if every iteration passed to a core has a unique seed, then any time we run the loops, the same results will obtain. To do this we 

1. Set an "outer" seed in the traditional way, i.e., using `set.seed()` outside `foreach`.

2. `sample()` "inner" seeds to be used in each iteration. The number of inner seeds is equal to the number of simulations/iterations (i.e., the max of your iteration index). Note that this sampling will always produce the same inner seeds, because we've set our outer seed in step (1). 

3. Set the seed within your simulation code using `set.seed()`, but using an indexed inner seed, where the index is the iteration number.

To make this more clear:

```{r, eval=T, echo=T, results='hide'}
ncores = 3
nsims = 10
cl = makeCluster(ncores)
registerDoParallel(cl)

set.seed(123)                 # Set seed in the usual way
inner_seeds = sample(         # Sample nsims-many inner seeds
  x = 999999, 
  size = nsims
  )

parallel_1 = 
  foreach(i = 1:nsims, 
          .combine = c
          ) %dopar% {
            
    set.seed(inner_seeds[i])  # Set seed for each iteration, using indexed seed
    rnorm(1) 
    
  }

parallel_2 = 
  foreach(i = 1:nsims, 
          .combine = c
          ) %dopar% {
            
    set.seed(inner_seeds[i])
    rnorm(1) 
  }

stopCluster(cl)

identical(parallel_1, parallel_2)
```

Now this is of course a bit hacky, but this approach does have practical utility. Namely, if you ever need nested parallel loops (using the `%:%` nesting operator detailed in the [`foreach` documentation](https://cran.r-project.org/web/packages/foreach/vignettes/nested.html)), then unfortunately the `%dorng%` backend is not supported. In this case, the only way I have found to successfully reproduce simulations is the above, iteration-level seed-setting approach. To be clear, this isn't setting the same exact seed on all cores. We made a vector of seeds, `inner_seeds`, and each simulation uses a seed from this vector before executing its instructions. You can think of this as setting a different seed across a bunch of different R sessions, and then combining the results; however, the universe of inner seeds remains the same, and therefore so does the results set across runs. 


\


# Thread Competition
***

Now we are running loops in parallel, and our pseudo-random numbers are reproducible as we would want. However, there is a potential lurking issue that will completely tank any simulations structured as we have here: other multit-hreaded/parallel tasks competing for CPU resources. In other words, if R is trying to execute simulations in parallel while also trying to execute some other task in parallel simultaneously, often times your computer will hang and eventually crash. 

So why would anyone be doing multiple forms of parallel processing simultaneously? The most common reason for this is when you are using optimized BLAS/LAPACK libraries while executing linear algebra-heavy simulations. This includes libraries like [Open BLAS](https://www.openblas.net/), [Intel's MKL](https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html), [AMD's Math Library](https://developer.amd.com/amd-aocl/amd-math-library-libm/), [Apple's Accelerate](https://developer.apple.com/documentation/accelerate) or [ATLAS BLAS](http://math-atlas.sourceforge.net/). In addition to having superior algorithms that are optimized for your specific hardware, these libraries often times have multi-threaded matrix operations; i.e., they do common matrix operations in parallel, such as matrix multiplication, inversion, etc. Thus, if you are using these libraries and forget to disable their inherent multi-threading, they will compete with `foreach` when accessing your CPUs, which will ultimately hang your system. For example, if your computer has 8 threads, and you tell R to give `foreach` 7 of them, but MKL reserves 4 of them for matrix operations, suddenly you're trying to access 11 threads when you only have 8. The only option is to use the same thread for multiple tasks simultaneously, but this isn't possible so the computer hangs. A particularly egregious case of this is known as a [FORK bomb](https://en.wikipedia.org/wiki/Fork_bomb). 

\

### Checking BLAS/LAPACK Threading
***

To check if your PC is running multi-threaded matrix operations, use the [`RhpcBLASctl`](https://cran.r-project.org/web/packages/RhpcBLASctl/index.html) package:
```{r, echo=T, eval=T}
library(RhpcBLASctl)
blas_get_num_procs()
omp_get_num_procs()
```

If either of these results is greater than 1, then you have multithreading enabled. We won't delve into the differences between BLAS and OMP here; suffice it so say that you want both of these at 1 for single-threaded matrix operations. <span style="color: red;">**IF YOU USE [MICROSOFT'S R-OPEN](https://mran.microsoft.com/open)**, **_then you definitely have MKL enabled as your BLAS/LAPACK library, and multi-threading is likely enabled by default_**</span>. 

Alternatively, you can check on the name of your BLAS library, which will tell you at least if its not the default library:

```{r, echo=T, eval=T}
sessionInfo()[c("BLAS", "LAPACK")]
```

but this isn't conclusive, as it merely tells us that I'm using MKL, not that MKL is currently set to use multiple threads. For that reason, the `RhpcBLASctl` method is probably superior.

\

### Multi-threaded Packages
***

In addition to multi-threaded matrix operations, there are other R packages that are multithreaded by default and will therefore compete for CPU resources. The number one culprit here is probably [`data.table`](https://cran.r-project.org/web/packages/data.table/index.html). However, other machine learning libraries have the option for multithreading as well, such as [`caret`](https://topepo.github.io/caret/), and [`glmnet`](https://cran.r-project.org/web/packages/glmnet/index.html). To my knowledge neither of these last two are multithreaded by default, so this only becomes a problem if you accidentally tell them to use more than one CPU. 

\
  
### Solving Thread Competition
***

Fortunately, it is extremely easy to solve thread competition.

1. To disable _multi-threaded matrix operations_, use the [`RhpcBLASctl`](https://cran.r-project.org/web/packages/RhpcBLASctl/index.html) package, **and be sure to disable multi-threading in your loop code as seen below, not just outside of it:**

    ```{r, echo=T, eval=T, results='hide'}
# disable in parent process 
library(RhpcBLASctl)
library(doRNG)

blas_set_num_threads(1)
omp_set_num_threads(1)

ncores = 3 
nsims = 3
cl = makeCluster(ncores)
registerDoParallel(cl)
set.seed(123)

# disable in child processes 
  foreach(i = 1:nsims,
          .packages = "RhpcBLASctl"   # Dont forget to export RhpcBLASCtl pkg
          ) %dorng% {
            
    library(RhpcBLASctl)
    blas_set_num_threads(1)           # DISABLE WITHIN LOOP HERE
    omp_set_num_threads(1)   
    
    rnorm(1) 
    
          }
  
stopCluster(cl)
    ```
<br>

2. To disable _packaged multithreading_, you'll just have to read the documentation for that package to determine how to set the max number of threads to 1. For example, browsing the documentation for `data.table` immediately tells us that the `setDTthreads()` function does what we want. 

\

### Caveats
***

There are a few qualifiers worth mentioning here. 

1. If you aren't using matrix operations in your parallel tasks, then multi-threaded matrix operations shouldn't cause problems in theory, because they aren't even being attempted. However, if they aren't happening, it also doesn't hurt to just set the number of threads to 1. Just make sure the change it back if you want to leverage that efficiency in your R session later on. 

2. If you aren't using the multi-threaded R packages, perhaps it goes without saying that you don't need to worry about them competing for resources in this way.

3. It is possible that some combination of matrix or package multithreading along with parallel simulations is "optimal," in the sense that its faster. For example, if you have 16 threads you could assign 5 threads to `foreach`, and then have each of those processes use 2 threads for matrix operations. You'd then have 15 threads in use, plus one for R-studio or other tasks on reserve. Perhaps this would be faster overall than single-threaded matrix operations on 15 simulation processes. I haven't done any testing for this, so I have no idea whether or when this would be better. At the extreme, it could in theory be better to use a single threaded simulation (i.e., serial simulations), while using all cores for matrix operations. I imagine that is an edge case, but again I cannot rule it out as I have not investigated this extensively. In any case, I'm lazy, so I just disable multi-threading on all packages and matrix operations, and give all my threads to `foreach`. 

\

# Parameter Grids 
***

Now that we're working in parallel without suffering thread competition and can consistently reproduce our results, the next step is to create a grid of parameters for our simulations, i.e., the points in the parameter space at which we'll investigate the behavior of our chosen procedure. These will of course be specific to the process under study, but its worth showing a general way to go about creating this grid. 

As a contrived toy example, suppose our task involves normal random variables with different means and variances. Specifically, we are interested in $X \sim \mathcal{N}\left(\mu \in \{0, -1 \}, \,\,\sigma^2 \in \{1, 3\}\right)$; that is, we'll vary the mean and variance of some random variable $X$. Additionally, we want to investigate how sample size affects the properties of these random draws. Right now its not important *why* we're doing this or what $X$ will be used for, only that we want random variables with every pairwise combination of these parameter values. 

To do so, we create vectors of these parameters, and then use `expand.grid()` to get the pairwise combinations:

```{r}
mu = c(0, -1)
sig2 = c(1, 3)
n = c(5, 10)

(grid = expand.grid(mu = mu, sig2 = sig2, n = n))
```


So now we have the parameters we need for our simulations in a nice tabular format, which we can then loop over when creating data from our chosen data generating process. For example (omitting the back-end setup, etc. for clarity):

```{r, eval=T, warning=F}
# SETUP OMITTED FOR SIMPLICITY. SEE EXAMPLES SECTION FOR FULL SETUP. 

out = foreach(i = 1:nrow(grid)
        ) %dopar% {
            
    
    rnorm( n = grid[i, "n"],
           mean = grid[i, "mu"],
           sd = sqrt(grid[i, "sig2"])
    )
    
        }

head(out)
```

The main point here is that we've created a `grid` of all unique pairwise parameter combinations, and we've looped over each row of that `grid` to then get random numbers using every possible parameter combination. In this way we've ensured that we simulate data from each of the desired parameterizations, all without having to manually specify the parameters, and without having to do any sort of nested looping, e.g. `for(i in mu){ for j in sig2 {...}}`. This is akin to a grid-search in optimization, although here we're not searching for an optimal set of parameters, we're investigating the behavior of some procedure at pre-defined grid points in the parameter space. 

\

### Extending Grids to Multiple Simulations
***

Of course, *we've only done one simulation* here. That is, at each set of parameter values, we have one iteration. But in Monte Carlo analysis, we want some large number of simulations in order to see how things are behaving, so we'll need to replicate this grid. Suppose we want 2 simulations instead of 1, then we can use `do.call()`, `cbind.data.frame`, and `replicate()` together to get a grid with each grid point appearing twice, so that we'll have two simulations at that point instead of one:

```{r}
nsims = 2 

mu = c(0, -1)
sig2 = c(1, 3)
n = c(5, 10)

grid = expand.grid(mu = mu, sig2 = sig2, n = n)
(grid_multi = do.call(rbind.data.frame, replicate(nsims, grid, simplify=F)))
```

And we can now see that each grid point appears twice, so if we were to loop over this in the same as as we did previously, we produce results for each parameter combination twice. Inductively then, we know that for arbitrary values of `nsims`, we'll get `nsims`-many results for each parameter combination, which is precisely what we want in Monte Carlo analysis. 

Briefly explaining the above code, `replicate()` is replicating the `grid` dataframe `nsims`-many times, and storing these replicates in a list. Then, `do.call` calls the `rbind.data.frame` function with each element of the replicated list as its arguments. Mathematically, `do.call` is effectively executing $f(x_1, x_2, ..., x_{nsims})$ where $f()$ is `rbind.data.frame`, and each `x_j` is an element of a list. In this case, the list-elements are all the original, single-simulation `grid` object.  

\

### Why not use a nested loop? 
***

One potential question is why not use a nested loop instead of replicating each grid point `nsims`-many times. That is, we could just repeatedly loop over the original `grid` object, rather than replicating it several times, like so (omitting the back-end setup, etc. for clarity): 

```{r, echo=T, eval=T, warning=F}
# SETUP OMITTED FOR CLARITY. SEE ENDING EXAMPLES FOR FULL SETUP.

# Make grid WITHOUT REPLICATING 
mu = c(0, -1)
sig2 = c(1, 1.5)
n = c(5, 10)

grid = expand.grid(mu = mu, sig2 = sig2, n = n)

# make number of simulations a variable for looping
nsims = 2

out = foreach(i = 1:nrow(grid)
        ) %dopar% {
            
    res = list()
    
    # create an inner, nested loop that repeatedly simulates from a gridpoint
    # instead of replicating the gridpoint and looping over the replicates
    for(j in seq_len(nsims)){

      res[[j]] = rnorm( n = grid[i, "n"],
                      mean = grid[i, "mu"],
                      sd = sqrt(grid[i, "sig2"])
                      )
    }
    
    return(res)
    
}

out[1:3]
```

The key difference here is that we create a single set of grid points without duplication (i.e., without using `replicate()`), and then create an inner loop--- `for(j in seq_len(nsims))`---that repeatedly uses these grid points `nsims`-many times. An obvious advantage of the nested-loop approach is that it conserves memory; the grid is not replicated, so each core only stores a copy of the original, single-simulation grid.  The memory gains are somewhat trivial though due to how R stores lists in memory, but that is a technical aside that I won't delve into now (see [this page](https://adv-r.hadley.nz/vectors-chap.html#lists) for more info). 


However, **the major disadvantage of the nested-loop approach is load balancing**. In most real-world applications we will be varying the sample size of our procedure, and by much more than we are doing so here; i.e., we'll estimate some model at small and large sample sizes. In doing so, the nested-loop approach becomes problematic for load balancing purposes, because the CPUs that have the small sample sizes will take way less time to complete their inner, nested loop (recall, the set of instructions on the right-hand side of `%dopar%` are executed on a single CPU). Eventually then, the CPUs with the large sample sizes will still be running their nested, inner-loops while the CPUs that had the smaller sample sizes have completed their tasks and are *sitting idle* doing nothing.
\

### Caveats 
***

In most cases, the replicated-grid approach will be better, often times by several hours when compared with the parallel-outer, serial-inner approach. However, there are at least two instances where this won't hold.


1. If the run-time of a single simulation is small, e.g., < 1 second, it is possible that the overhead from parallelizing at the simulation level---rather than the grid-point level---will exceed the gains from parallelizing. This is known as [parallel slow down](https://en.wikipedia.org/wiki/Parallel_slowdown). It's possible to mitigate this by using prescheduling, but even this won't always fix the issue if single simulations take a very small amount of time.  Prescheduling splits the total number of tasks into chunks (at random) and assigns these tasks to each CPU. So if you allocate 10 CPUs, and there are 100 tasks, the computer will randomly select 10 tasks to assign to each CPU. Without prescheduling, when a CPU finishes a task, it will communicate with the other CPUs in the network or the parent process and determine what tasks still need doing, and then it does one of the ones that remains. So, by prescheduling we reduce some of this communication overhead. There are advantages and disadvantages to prescheduling that I won't go into here, but a general rule is if your tasks are numerous and require little time individually, use prescheduling, otherwise don't. To enable presheduling in `foreach`, do the following:

    ```{r, eval=F}

    foreach(i = 1:nsims, 
        .options.multicore = list(preschedule = TRUE)) 
        %dorng% {
          
          [...code...]
        
        }
    ```

 

2. If you are doing some form of Markov Chain Monte Carlo (MCMC), rather than traditional Monte Carlo analysis, the nested loop approach will generally become **_necessary_**. For things like the Metropolis-Hastings algorithm and its derivatives (e.g., Metropolis, Gibbs), we are typically sampling from a full-conditional distribution that depends on some previous iteration. So parallelization here would occur at the chain-level, and within each chain we would have serial loops to sample from the full conditionals. The gridding approach would also not make as much sense anyway, as the proposal parameters are not fixed ex-ante aside from the starting values.

\

# Using Functions Properly 

***

At this point we've set up our environment, and are ready to start with the actual simulations. To recap, we are using `foreach` combined with `%dopar%` or `%dorng%` for parallelism, we've ensured reproducability, we have handled thread competition with other parallel processes, and now we have a grid of parameter values at which to investigate the behavior of some procedure, with each grid value appearing once for each simulation iteration. Now we can make some functions that will automate the simulation and transformation of our data.

R is an object oriented, functional programming language. We should leverage both of these facts to make our simulations easier to understand, implement, and debug. At a minimum, this will consist of having two functions: 

1. A **simulation function** that creates the data to be further analyzed. 

2. An **estimation/analysis function** that estimates the model(s) that we want, and tidies the results for us. 

We could optionally further separate the estimation function (2) into two functions, one for estimation and the other for tidying, rather than combining these steps. 

This approach to Monte Carlo analysis---having a simulation function, and an estimation function---is one of the most important reasons why I choose not to use the parallelized `apply` family of functions. If you're unfamiliar with the `apply` family of functions, you can either skip this paragraph or [familiarize yourself](https://www.r-bloggers.com/2015/07/r-tutorial-on-the-apply-family-of-functions/) with them. Because our data must be both simulated and then used in estimation, the `apply` family of functions requires that there be either (i) a single function that does both of these tasks (e.g., via nesting), or (ii) that we pre-simulate every single dataset that will be needed for the estimation procedure, and then call the estimation function on each of them using `apply` functionality. The first approach is bad because debugging functions becomes more nightmarish as they get more complicated, so smooshing two functions into one is ill-advised. The second approach is unfeasible in most interesting cases because there is usually not enough memory to store hundreds of thousands of pre-simulated datasets. Thus, because we'll compartmentalize the simulation and estimation into two functions, the `foreach` approach becomes the natural one to implement. 

\

### Simulation Function 
***

The goal of the simulation function is to take in one row from our parameter grid, and use those parameters to create a dataset that will eventually be transformed via estimation or some other procedure of interest. Keeping with the toy example used previously, lets suppose we are just simulating some realizations of a normal random variable at various grid points and sample sizes. We have our grid from earlier: 

```{r}
print(grid_multi)
```

which has 3 parameters, `mu`, `sig2` and `n`. So our simulation function will have to take in *at least* these parameters, and potentially others. It is my view that the easiest way to do this is just to take in a vector of parameters as a single parameter to the function, rather than explicitly naming each parameter to the function. This way, if the grid columns change, we don't have to then go and add or remove parameters from the simulation function. 

For example, we could specify the parameters explicitly, then index our grid by row when passing the arguments (in this case, just row 1): 

```{r}
simulate_fn = function(mu, sig2, n){
  
  return(
    list(
      mu = mu, 
      sig2 = sig2, 
      n = n
    )
  )
  
}

simulate_fn(
  mu = grid_multi[1, "mu"], 
  sig2 = grid_multi[1, "sig2"],
  n = grid_multi[1, "n"]
  )
```

But if we later decide we want to add new parameter to the grid, we then have to update the `simulate_fn` function accordingly. Updating the function's parameters can be avoided if we just pass a named parameter vector as a single parameter, and unpack it inside the function:

```{r}
simulate_fn = function(grid_row){
  
  grid_row = unlist(grid_row) # re-class to vector from data.frame

  return(
    list(
      mu = grid_row["mu"], 
      sig2 = grid_row["sig2"], 
      n = grid_row["n"]
    )
  )
  
}

simulate_fn(grid_multi[1,])
```

\

Using this second approach then, we can define a function to simulate the normal realizations, and see what it produces for a single iteration on our grid: 

```{r}
simulate_fn = function(grid_row){
  
  grid_row = unlist(grid_row)
  
  rnorm(
    n = grid_row["n"], 
    mean = grid_row["mu"],
    sd = sqrt(grid_row["sig2"])
  )
  
}

simulate_fn(grid_row = grid_multi[1,])
```
which is a numeric vector, as we would expect. Now we can do whatever transformation of this data to obtain statistics of interest for each iteration. However, we'll need an estimation function first. 

\ 

### Estimation Function 
***
The goal of the estimation function is to take in the data simulated from the simulation function (`simulate_fn()`, previously), and carry out whatever procedure on that data we are interested in analyzing via the Monte Carlo method. To keep up the toy example, lets say now that we're interested in the estimators of the first and second moments of a normal random variable after we've exponentiated it (that is, of the log-normal distribution). So we'll need a function that takes in the data, exponentiates it, and computes these estimates. **Importantly,** we want to know how the various parameters under consideration impact these estimators (if at all), so we need to also keep track of the parameters that we're using to generate the data for each iteration. One possibility is the following, where we again pass in the data-generating grid row:

```{r}

estimate_fn = function(dat, grid_row){
  
  dat_exp = exp(dat)
  xbar = mean(dat_exp)
  s2   = var(dat_exp)
  
  out = c(xbar = xbar, s2 = s2, grid_row)
  
  return(out)
}
```

\

So now we'll simulate some data using `simulate_fn` at a single grid-point, `grid_multi[1,]`, and return the quantities of interest from this data using the `estimate_fn()`:

```{r}
grid_row = unlist(grid_multi[1,])

dat = simulate_fn(grid_row = grid_row)

estimate_fn(
  dat = dat, 
  grid_row = grid_row
)
```
which gives us the sample mean and variance for a single iteration of transforming data to log-normal realizations, as well as stores the data-generating parameters for use in post-simulation analysis. Inductively, we can imagine doing this for all rows in our grid, which would give us results for all parameter comibinations multiple times. We could then analyze these results graphically or otherwise. The next section puts all of this together in a comprehensive toy example. 

\ 

# Putting it All Together
*** 

Before we jump into some more serious examples, we can combine everything we've learned using the toy example, just to give an idea about the structure. I'm not going to separate the code out for each step we've covered; all of it will appear in a single chunk with comments so as to mimic what we would actually have in a script. I'll also be doing things in a slightly different order than was presented in the guide for computational and presentation purposes. This is again for the example of analyzing the estimators of the first two moments of the log-normal distribution under various parameterizations. I'll do 30 simulations at each grid point for demonstration purposes, but in reality you'd probably want more. 

```{r}
# Load up packages 
pks = c("doParallel", "doRNG", "RhpcBLASctl")
needed = setdiff(pks, rownames(installed.packages()))

if(length(needed) > 1)
  install.packages(pks, Ncpus = parallel::detectCores() - 1)

invisible(sapply(pks, library, character.only=T))

# Define Simulation and Estimation Functions
simulate_fn = function(grid_row){
  
  rnorm(
    n = grid_row["n"], 
    mean = grid_row["mu"],
    sd = sqrt(grid_row["sig2"])
  )
  
}

estimate_fn = function(dat, grid_row){
  
  dat_exp = exp(dat)
  xbar = mean(dat_exp)
  s2   = var(dat_exp)
  
  out = c(xbar = xbar, s2 = s2, grid_row)
  
  return(out)
}

# Make the parameter grid 
nsims = 300 

mu = c(0, -1)
sig2 = c(1, 3)
n = c(5, 10)

grid = expand.grid(mu = mu, sig2 = sig2, n = n)
grid_multi = do.call(rbind.data.frame, replicate(nsims, grid, simplify=F))

# Disable BLAS/LAPACK multithreading in parent process 
blas_set_num_threads(1)
omp_set_num_threads(1)

# Spin up backend
nprocs = parallel::detectCores() - 1
os = .Platform$OS.type
type = if(grepl('unix', os)) 'FORK' else 'PSOCK'
cl = makeCluster(nprocs, type = type)
registerDoParallel(cl)

# Run sims 
set.seed(123)

res = foreach(i = seq_len(nrow(grid_multi)),
              .packages = 'RhpcBLASctl',
              .errorhandling = "pass",
              .inorder = F,
              .options.multicore = list(preschedule=T)
              ) %dorng% {
    
  blas_set_num_threads(1)
  omp_set_num_threads(1)
  
  grid_row = unlist(grid_multi[i,])
    
  dat = simulate_fn(grid_row)
  out = estimate_fn(dat, grid_row)
  
  return(out)
  
}

stopCluster(cl)

head(res)
```

\


# Example: Bias of AR(1) Model 
***

In this example I will utlize the steps outlined previously to show that order-1 time-series autoregressive models (AR(1) models) have biased estimators of the autoregressive parameter. This can be shown theoretically, as well, but pretend for the moment that we don't know such a closed form solution exists, or perhaps we're too lazy to find one. The Monte Carlo method is well suited for the task. 

Briefly, AR(1) models assume a data generating process of the form $$ y_t = \rho y_{t-1} + \varepsilon_t $$ where $t = 1, 2, ..., T$ indexes a time period, $|\rho| < 1$, and for the time being we'll assume that $\varepsilon_t \overset{iid}{\sim} \mathcal{N}(0, 1)$, although with large enough sample sizes normality is not required. Then we can estimate $\rho$ by $$\hat\rho = \frac{\sum_t (y_t - \bar y)(y_{t-1} - \bar y)}{\sum_t (y_{t-1} - \bar y)^2}$$ that is, by using its (ordinary) least squares estimator (which is also the sample autocorrelation function). 

Using the Monte Carlo method, we will show that $\E(\hat\rho) - \rho \neq 0$, meaning that $\hat\rho$ is biased. In fact, we'll determine that $|\E(\hat\rho)| < |\rho|$, meaning that the AR parameter is biased towards zero. 

In the below, I will simply apply what was discussed in the previous sections without further discussion. The intention here is to give example code for more complicated problems, but the principles in application are identical to those already discussed. 

```{r, cache=T}
# Install, load necessary packages
pks = c("RhpcBLASctl", "doParallel", "doRNG")
needed = setdiff(pks, rownames(installed.packages()))
if(length(needed) > 0)
  install.packages("pks", Ncpus = parallel::detectCores() - 1)

invisible(sapply(pks, library, character.only=T))

n = c(30, 50, 100, 500, 10000)
p = c(.2, .5, .75, .95)

nsims = 500
grid = expand.grid(n = n, p = p)
grid_multi = do.call(rbind.data.frame, replicate(nsims, grid, simplify = F))

simulation_fn = function(grid_row){
  
  grid_row = unlist(grid_row)
  
  rho = grid_row["p"]
  
  y = list()
  y[[1]] = rnorm(1)
  
  for(j in 2:grid_row["n"]) {
    y[[j]] = rnorm(1, mean = rho*y[[j-1]])
  }
    
  return(
    data.frame(
      y = unlist(y),
      lag_y = c(NA, head(unlist(y), -1))
    )
  )
  
}

estimation_fn = function(dat, grid_row){
  
  grid_row = unlist(grid_row)
  mod = lm(y ~ lag_y, data = dat)
  
  return(
    
    c(coef(mod), grid_row)
    
    )
  
}

RhpcBLASctl::blas_set_num_threads(1)
RhpcBLASctl::omp_set_num_threads(1)

ncpus = parallel::detectCores()-1
type = if(.Platform$OS.type == 'unix') 'FORK' else 'PSOCK'
cl = makeCluster(ncpus, type=type)
registerDoParallel(cl)


out = 
  foreach(
  i = seq_len(nrow(grid_multi)),
  .options.multicore = list(preschedule=T),
  .maxcombine = nrow(grid_multi),
  .multicombine = T,
  .packages = 'RhpcBLASctl'
) %dorng% {
  
  RhpcBLASctl::blas_set_num_threads(1)
  RhpcBLASctl::omp_set_num_threads(1)
          
  grid_row = grid_multi[i, ]
  
  dat = simulation_fn(grid_row)
  
  return(estimation_fn(dat, grid_row))
  
}


stopCluster(cl)
rm(cl)

out2 = 
  foreach(
  i = seq_len(nrow(grid_multi)),
  .options.multicore = list(preschedule=T),
  .maxcombine = nrow(grid_multi),
  .multicombine = T,
  .packages = 'RhpcBLASctl'
) %do% {
  
  RhpcBLASctl::blas_set_num_threads(1)
  RhpcBLASctl::omp_set_num_threads(1)
          
  grid_row = grid_multi[i, ]
  
  dat = simulation_fn(grid_row)
  
  return(estimation_fn(dat, grid_row))
  
}


```


# Example 2: Inefficiency of Heckman 2-Step

```{r, echo=T, eval=T, cache=T}
### Define custom regression function
fiml_reg = function(formula, data){
  
  fr = model.frame(formula, data = data)
  X = model.matrix(formula, fr)
  y = model.response(fr)
  
  starts = c( 
    coef(lm(y ~ X + 0)),
    1
  )
  
  mod = nlm(fiml_ll, p = starts, hessian = T, X=X, y = y)
  se = sqrt(diag(solve(mod$hessian)))
  
  out = cbind(mod$estimate, se)
  return(out)
  
}


### Likelihood to custom reg fn
fiml_ll = function(parms,  y, X, tau = 0){
  
  b = head(parms, -1)
  sig = tail(parms, 1)

  obs =  y > tau
  
  y_obs = y[obs]
  y_cen = y[!obs]
  eta = as.vector(X %*% b)
  eta_obs = eta[obs]
  eta_cen = eta[!obs]
  
  ll_uncens = sum(dnorm((y_obs - eta_obs)/sig, log=T) - log(sig))
  ll_cens = sum(pnorm((tau - eta_cen)/sig, log.p = T))
  
  ll = -(ll_uncens + ll_cens)
  
  # if(is.na(ll))
  #   ll = .Machine$double.xmax
  # 
  # cat(ll, "\n")
  return(ll)
  
}


### Simulation function
simulation_fn = function(grid_row) {
  n = grid_row[["n"]]
  b1 = grid_row[["b1"]]
  
  b = c(b1, -1.2, .15)

  X = cbind(
    1, 
    rnorm(n, sd = 2.5),
    rexp(n, .33)
  )
  
  y = rnorm(n, mean = X%*%b, sd = 1.75)
  sum(y < 0)

  obs =  y > 0
  y[obs == 0] = 0
  
  out = cbind.data.frame(X = X, y = y, obs = obs)
  return(out)
    
}


#### Estimation function
estimation_fn = function(dat, grid_row, iter){
  
  grid_row = unname(unlist(grid_row))
  
  # Model 1 - OLS
  ols = lm(y ~ X.2 + X.3, data=dat)
  
  # Model 2 - Heckman
  hecksel = suppressWarnings(
    glm(obs ~ X.2 + X.3,
        family=binomial(link="probit"),
        data = dat
        )
  )
  
  eta = predict(hecksel)
  
  imills = dnorm(eta)/pnorm(eta)
  
  heck2step = lm(y ~ X.2 + X.3 + imills,
                 data = dat,
                 subset = obs == 1
                 )
  
  # Model 3 - FIML Tobit 1
  out_fiml = tryCatch(
    fiml_reg(y ~ X.2 + X.3, data = dat)[1:3, 1:2], 
    error = function(e) NULL
    )
  
  if(is.null(out_fiml))
    return(NULL)
  
  # Tidy everything
  prop_obs = sum(dat$obs) / nrow(dat)
  
  out_2step = summary(heck2step)$coefficients[1:3, 1:2]
  out_ols = summary(ols)$coefficients[1:3, 1:2]
  
  colnames(out_2step) = colnames(out_ols) = colnames(out_fiml) =  c("est", "se")
  rownames(out_2step) = rownames(out_ols) = rownames(out_fiml) = c("b1", "b2", "b3")

  model = rep(c("ols", "2step", "fiml"), each=nrow(out_2step))

  out = cbind.data.frame(
    model = model,
    rbind.data.frame(out_ols, out_2step, out_fiml)
    )

  rownames(out) = NULL
 
  
  data.frame(
    iter_id = iter,
    param = rep(rownames(out_2step), 3),
    n = grid_row[1], 
    b1 = grid_row[2], 
    prop_obs = prop_obs,
    out
  )
  
  
}




### Install, load necessary packages
pks = c("RhpcBLASctl", "doParallel", "doRNG")
needed = setdiff(pks, rownames(installed.packages()))
if(length(needed) > 0)
  install.packages("pks", Ncpus = parallel::detectCores() - 1)

invisible(sapply(pks, library, character.only=T))


### Set up for simulations
n = c(50, 500, 5000)
b1 = c(1, 2, 3)

nsims = 500
grid = expand.grid(n = n, b1 = b1)
chunks = seq(from = nsims, to = nsims*nrow(grid), by = nsims)

RhpcBLASctl::blas_set_num_threads(1)
RhpcBLASctl::omp_set_num_threads(1)

ncpus = parallel::detectCores()-1
type = if(.Platform$OS.type == 'unix') 'FORK' else 'PSOCK'
cl = makeCluster(ncpus, type=type)
registerDoParallel(cl)


### Main simulations
out = 
  foreach(
  i = seq_len(nrow(grid)*nsims),
  .options.multicore = list(preschedule=T),
  .maxcombine = nrow(grid)*nsims,
  .multicombine = T,
  .packages = c('RhpcBLASctl')
) %dorng% {
  
  RhpcBLASctl::blas_set_num_threads(1)
  RhpcBLASctl::omp_set_num_threads(1)
          
  grid_row = grid[which.max(i <= chunks),]
  
  dat = simulation_fn(grid_row)
  
  estimation_fn(dat, grid_row, i)
  
}
  
stopCluster(cl)

  

```

# Example 3: Small Sample Bias of Logit Models 

```{r, cache=T}
simulation_fn = function(grid_row){
  b1 = grid_row[["b1"]]
  n = grid_row[["n"]]
  
  X = cbind(
    1,
    runif(n, -5, 5),
    rbinom(n, 1, .5),
    rexp(n, .33),
    rbeta(n, 12, 4),
    rt(n, 5),
    rnorm(n, sd=1.75),
    rpois(n, 4)
    )
  
  b = c(-1, b1, 2, -.25, 1.2, .8, -1.5, .33)
  eta = X%*%b
  
  y = rbinom(n, 1, plogis(eta))
  
  dat = cbind.data.frame(y = y, X = X[,-1])
  
  return(dat)
}

estimation_fn = function(data, grid_row, iter_id){
  
  grid_row = unlist(grid_row)
  
  mod_bad = suppressWarnings(
    glm(y ~ . , 
        data = dat, 
        family=binomial(link="logit")
        )
    )
  mod_good = suppressWarnings(
    glm(y ~ . , 
        data = dat, 
        method = "brglmFit", 
        type = 'AS_mean', 
        family=binomial(link="logit")
        )
    )
  
  
  if(!(mod_bad$converged && mod_good$converged))
    return(NULL)
  
  out_bad = summary(mod_bad)$coefficients[2, 1:2]
  out_good = summary(mod_good)$coefficients[2, 1:2]
  
  out = rbind(out_bad, out_good)
  
  cbind.data.frame(
    iter_id = iter_id,
    n = grid_row[["n"]],
    b1 = grid_row[["b1"]],
    model = c("glm", "pml"),
    out
    )

  
}



### Install, load necessary packages
pks = c("RhpcBLASctl", "doParallel", "doRNG", "brglm2")
needed = setdiff(pks, rownames(installed.packages()))
if(length(needed) > 0)
  install.packages("pks", Ncpus = parallel::detectCores() - 1)

invisible(sapply(pks, library, character.only=T))


nsims = 500
b1 = c(-.5, 1.2, 2.7, 3.3)
n = c(70, 100, 150, 200)
grid = expand.grid(b1 = b1, n = n)
chunks = seq(from = nsims, to = nsims*nrow(grid), by = nsims)

RhpcBLASctl::blas_set_num_threads(1)
RhpcBLASctl::omp_set_num_threads(1)

ncpus = parallel::detectCores()-1
type = if(.Platform$OS.type == 'unix') 'FORK' else 'PSOCK'
cl = makeCluster(ncpus, type=type)
registerDoParallel(cl)

out = 
  foreach(
  i = seq_len(nrow(grid)*nsims),
  .options.multicore = list(preschedule=T),
  .maxcombine = nrow(grid)*nsims,
  .multicombine = T,
  .packages = c('RhpcBLASctl', 'brglm2')
) %dorng% {
  
  
  library(brglm2)
  RhpcBLASctl::blas_set_num_threads(1)
  RhpcBLASctl::omp_set_num_threads(1)
          
  grid_row = grid[which.max(i <= chunks),]
  
  dat = simulation_fn(grid_row)
  
  estimation_fn(dat, grid_row, i)
  
}
  
stopCluster(cl)


```